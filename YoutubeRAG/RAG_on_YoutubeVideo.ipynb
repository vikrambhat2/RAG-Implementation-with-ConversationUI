{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variable to hold the vector database\n",
    "global_vector_db = None\n",
    "\n",
    "# Function to process the YouTube URL and create the vector store\n",
    "def process_youtube_url(youtube_url):\n",
    "    loader = YoutubeLoader.from_youtube_url(youtube_url, add_video_info=False)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "\n",
    "    # Split the documents and keep metadata\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "    # Ingest into vector database\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to answer questions based on the vector store and chat history\n",
    "# Submit URL function\n",
    "def submit_url(youtube_url):\n",
    "    global global_vector_db\n",
    "    try:\n",
    "        # Process the YouTube URL\n",
    "        global_vector_db = process_youtube_url(youtube_url)\n",
    "        \n",
    "        # Default question for summarization\n",
    "        default_question = \"Summarize this video.\"\n",
    "        chat_history = []  # Empty chat history for the first question\n",
    "        \n",
    "        # Get the summary\n",
    "        summary, _ = answer_question(default_question, chat_history)\n",
    "        \n",
    "        # Status message\n",
    "        status_message = \"Video indexed successfully ✅! You can now ask questions about the video in the chatbot.\"\n",
    "    except Exception as e:\n",
    "        # Handle errors and display in the status box\n",
    "        status_message = f\"❌ Error processing the video: {str(e)}\"\n",
    "        summary = \"\"  # No summary in case of error\n",
    "    \n",
    "    # Return the status message and the summary\n",
    "    return status_message, summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_question(question, chat_history):\n",
    "    global global_vector_db\n",
    "    if global_vector_db is None:\n",
    "        return \"Please process a YouTube video URL first.\", chat_history\n",
    "\n",
    "    try:\n",
    "        local_llm = 'llama3.2'\n",
    "        llama3 = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "        retriever = global_vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a video assistant tasked with answering questions based on the provided YouTube video context. \"\n",
    "            \"Use the given context to provide accurate, concise answers in three sentences. \"\n",
    "            \"If the context does not contain the answer, say you are not sure \"\n",
    "            \"Context: {context}\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        question_answer_chain = create_stuff_documents_chain(llama3, prompt)\n",
    "        chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "        response = chain.invoke({\"input\": question})\n",
    "        \n",
    "        # Ensure the response contains the expected \"answer\" key\n",
    "        if \"answer\" not in response:\n",
    "            raise ValueError(\"Response does not contain an 'answer' key.\")\n",
    "        \n",
    "        # Append the question and answer to chat history as a tuple\n",
    "        chat_history.append((question, response['answer']))\n",
    "        \n",
    "        # Return the answer and updated chat history\n",
    "        return response['answer'], chat_history\n",
    "    except Exception as e:\n",
    "        # Return error message in case of failure\n",
    "        error_message = f\"Error: {str(e)}\"\n",
    "        chat_history.append((question, error_message))\n",
    "        return error_message, chat_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradio interface function to ask a question\n",
    "def ask_question(question, chat_history):\n",
    "    response, updated_chat_history = answer_question(question, chat_history)\n",
    "    return updated_chat_history, updated_chat_history  # Return the updated chat history for gr.Chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"<h1 style='text-align: center; color: #4A90E2;'>YouTube Video Q&A</h1>\")\n",
    "        gr.Markdown(\"<p style='text-align: center;'>Enter a YouTube video URL to extract information and ask questions about it.</p>\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                youtube_url = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"Enter the YouTube video URL here...\", lines=1)\n",
    "                submit_btn = gr.Button(\"Submit URL\", variant=\"primary\")\n",
    "                \n",
    "                # Status box for indexing completion or error messages\n",
    "                status_info = gr.Textbox(\n",
    "                    label=\"Status Info\", \n",
    "                    placeholder=\"Indexing status will appear here...\", \n",
    "                    interactive=False, \n",
    "                    lines=2\n",
    "                )\n",
    "\n",
    "                # Summary box for video summary\n",
    "                summary_box = gr.Textbox(\n",
    "                    label=\"Video Summary\", \n",
    "                    placeholder=\"Summary will appear here...\", \n",
    "                    interactive=False, \n",
    "                    lines=6\n",
    "                )\n",
    "                \n",
    "                # Link submit button to the submit_url function\n",
    "                submit_btn.click(fn=submit_url, inputs=youtube_url, outputs=[status_info, summary_box])\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                chat_history = gr.Chatbot()\n",
    "                question = gr.Textbox(label=\"Your Question\", placeholder=\"Ask a question about the video...\", lines=1)\n",
    "                ask_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "                state = gr.State([])\n",
    "\n",
    "                # Link ask button to the ask_question function\n",
    "                ask_btn.click(fn=ask_question, inputs=[question, state], outputs=[chat_history, state])\n",
    "                \n",
    "                # Clear chat history\n",
    "                clear_btn.click(fn=lambda: ([], []), inputs=[], outputs=[chat_history, state])  # Clear chat history\n",
    "\n",
    "        # Add a footer or additional information\n",
    "        gr.Markdown(\"<footer style='text-align: center; margin-top: 20px;'>© Vikram Bhat</footer>\")\n",
    "\n",
    "    return demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "interface = create_gradio_interface()\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
