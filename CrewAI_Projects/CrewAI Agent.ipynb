{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting crewai\n",
      "  Using cached crewai-0.83.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting appdirs>=1.4.4 (from crewai)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting auth0-python>=4.7.1 (from crewai)\n",
      "  Using cached auth0_python-4.7.2-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting chromadb>=0.5.18 (from crewai)\n",
      "  Using cached chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from crewai) (8.1.7)\n",
      "Collecting crewai-tools>=0.14.0 (from crewai)\n",
      "  Using cached crewai_tools-0.14.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting instructor>=1.3.3 (from crewai)\n",
      "  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting json-repair>=0.25.2 (from crewai)\n",
      "  Using cached json_repair-0.30.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonref>=1.1.0 (from crewai)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain>=0.2.16 (from crewai)\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting litellm>=1.44.22 (from crewai)\n",
      "  Downloading litellm-1.53.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting openai>=1.13.3 (from crewai)\n",
      "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from crewai) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.22.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from crewai) (1.24.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from crewai) (1.24.0)\n",
      "Collecting pdfplumber>=0.11.4 (from crewai)\n",
      "  Using cached pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pydantic>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from crewai) (2.6.3)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from crewai) (1.0.1)\n",
      "Collecting pyvis>=0.3.2 (from crewai)\n",
      "  Using cached pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting regex>=2024.9.11 (from crewai)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tomli-w>=1.1.0 (from crewai)\n",
      "  Using cached tomli_w-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tomli>=2.0.2 (from crewai)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting uv>=0.4.25 (from crewai)\n",
      "  Downloading uv-0.5.5-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from auth0-python>=4.7.1->crewai) (3.9.3)\n",
      "Collecting cryptography<44.0.0,>=43.0.1 (from auth0-python>=4.7.1->crewai)\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from auth0-python>=4.7.1->crewai) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from auth0-python>=4.7.1->crewai) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from auth0-python>=4.7.1->crewai) (2.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (1.2.1)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (4.10.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (0.45b0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (1.62.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (29.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (8.3.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (3.9.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from chromadb>=0.5.18->crewai) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from chromadb>=0.5.18->crewai) (13.7.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from crewai-tools>=0.14.0->crewai) (4.12.3)\n",
      "Collecting docker>=7.1.0 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting docx2txt>=0.8 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting embedchain>=0.1.114 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached embedchain-0.1.125-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting lancedb>=0.5.4 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached lancedb-0.16.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting pyright>=1.1.350 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pyright-1.1.389-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pytest>=8.0.0 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pytest-8.3.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pytube>=15.0.0 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting selenium>=4.18.1 (from crewai-tools>=0.14.0->crewai)\n",
      "  Using cached selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting docstring-parser<0.17,>=0.16 (from instructor>=1.3.3->crewai)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.4 (from instructor>=1.3.3->crewai)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jiter<0.7,>=0.6.1 (from instructor>=1.3.3->crewai)\n",
      "  Downloading jiter-0.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic>=2.4.2 (from crewai)\n",
      "  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core<3.0.0,>=2.18.0 (from instructor>=1.3.3->crewai)\n",
      "  Downloading pydantic_core-2.27.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb>=0.5.18->crewai)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from langchain>=0.2.16->crewai) (2.0.30)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain>=0.2.16->crewai) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain>=0.2.16->crewai)\n",
      "  Using cached langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain>=0.2.16->crewai)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from langchain>=0.2.16->crewai) (0.1.85)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from litellm>=1.44.22->crewai) (7.0.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from litellm>=1.44.22->crewai) (4.23.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from litellm>=1.44.22->crewai) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.13.3->crewai) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.13.3->crewai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
      "Collecting typing-extensions>=4.5.0 (from chromadb>=0.5.18->crewai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl>=3.1.5->crewai) (1.1.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from opentelemetry-api>=1.22.0->crewai) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.63.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Downloading protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-api>=1.22.0 (from crewai)\n",
      "  Using cached opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-sdk>=1.22.0->crewai)\n",
      "  Using cached opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber>=0.11.4->crewai)\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfplumber>=0.11.4->crewai) (10.2.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber>=0.11.4->crewai) (3.3.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic>=2.4.2->crewai) (0.6.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from pyvis>=0.3.2->crewai) (8.22.2)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis>=0.3.2->crewai)\n",
      "  Using cached jsonpickle-4.0.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from pyvis>=0.3.2->crewai) (3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.5->auth0-python>=4.7.1->crewai) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.13.3->crewai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.13.3->crewai) (1.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4>=4.12.3->crewai-tools>=0.14.0->crewai) (2.5)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from build>=1.0.3->chromadb>=0.5.18->crewai) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from build>=1.0.3->chromadb>=0.5.18->crewai) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai) (1.16.0)\n",
      "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading cohere-5.12.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-cloud-aiplatform<2.0.0,>=1.26.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_cloud_aiplatform-1.73.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading langchain_cohere-0.3.3-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting langchain-community<0.4.0,>=0.3.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mem0ai<0.2.0,>=0.1.29 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading mem0ai-0.1.34-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pypdf<6.0.0,>=5.0.0 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb>=0.5.18->crewai) (0.36.3)\n",
      "INFO: pip is looking at multiple versions of googleapis-common-protos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb>=0.5.18->crewai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb>=0.5.18->crewai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.18->crewai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from importlib-metadata>=6.8.0->litellm>=1.44.22->crewai) (3.17.0)\n",
      "Requirement already satisfied: decorator in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.14.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.4->instructor>=1.3.3->crewai) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.22->crewai) (0.19.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (2.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (3.2.2)\n",
      "Collecting deprecation (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: nest-asyncio~=1.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai) (1.6.0)\n",
      "Collecting pylance==0.19.2 (from lancedb>=0.5.4->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pylance-0.19.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pyarrow>=12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pylance==0.19.2->lancedb>=0.5.4->crewai-tools>=0.14.0->crewai) (15.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain>=0.2.16->crewai) (1.33)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain>=0.2.16->crewai)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain>=0.2.16->crewai) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (24.3.25)\n",
      "Requirement already satisfied: sympy in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.12.1)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-grpc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.18->crewai)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb>=0.5.18->crewai)\n",
      "  Downloading grpcio-1.68.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (0.45b0)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-instrumentation-fastapi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.5.18->crewai)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Using cached opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai)\n",
      "  Using cached opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.18->crewai) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from posthog>=2.4.0->chromadb>=0.5.18->crewai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from posthog>=2.4.0->chromadb>=0.5.18->crewai) (2.2.1)\n",
      "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting iniconfig (from pytest>=8.0.0->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=8.0.0->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->chromadb>=0.5.18->crewai) (3.0.0)\n",
      "Collecting trio~=0.17 (from selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from tokenizers>=0.13.2->chromadb>=0.5.18->crewai) (0.23.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from typer>=0.9.0->chromadb>=0.5.18->crewai) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.18->crewai) (12.0)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading fastavro-1.9.7-cp310-cp310-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (4.9)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_cloud_bigquery-3.27.0-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_cloud_resource_manager-1.13.1-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting shapely<3.0.0dev (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading shapely-2.0.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.18->crewai) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.18->crewai) (2024.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain>=0.2.16->crewai) (2.4)\n",
      "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2.0.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (0.9.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.18->crewai) (0.1.2)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2024.1)\n",
      "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached qdrant_client-1.12.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain>=0.2.16->crewai)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from trio~=0.17->selenium>=4.18.1->crewai-tools>=0.14.0->crewai) (2.4.0)\n",
      "Collecting outcome (from trio~=0.17->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium>=4.18.1->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from stack-data->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.18->crewai) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (0.9.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading google_crc32c-1.6.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (2024.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.18->crewai) (0.6.0)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading grpcio_tools-1.68.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (5.3 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: setuptools in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from grpcio-tools>=1.41.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (70.3.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vikrambhat/Library/Python/3.10/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai) (1.0.0)\n",
      "INFO: pip is looking at multiple versions of googleapis-common-protos[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.29->embedchain>=0.1.114->crewai-tools>=0.14.0->crewai)\n",
      "  Using cached hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached crewai-0.83.0-py3-none-any.whl (215 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached auth0_python-4.7.2-py3-none-any.whl (131 kB)\n",
      "Using cached chromadb-0.5.20-py3-none-any.whl (617 kB)\n",
      "Downloading chroma_hnswlib-0.7.6-cp310-cp310-macosx_11_0_arm64.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached crewai_tools-0.14.0-py3-none-any.whl (462 kB)\n",
      "Downloading instructor-1.7.0-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached json_repair-0.30.2-py3-none-any.whl (18 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading litellm-1.53.1-py3-none-any.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
      "Using cached opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n",
      "Using cached pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Downloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached tomli_w-1.1.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading uv-0.5.5-py3-none-macosx_11_0_arm64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cryptography-43.0.3-cp39-abi3-macosx_10_9_universal2.whl (6.2 MB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached embedchain-0.1.125-py3-none-any.whl (211 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading jiter-0.6.1-cp310-cp310-macosx_11_0_arm64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpickle-4.0.0-py3-none-any.whl (46 kB)\n",
      "Using cached lancedb-0.16.0-cp38-abi3-macosx_11_0_arm64.whl (22.6 MB)\n",
      "Using cached pylance-0.19.2-cp39-abi3-macosx_11_0_arm64.whl (26.7 MB)\n",
      "Using cached langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.68.0-cp310-cp310-macosx_12_0_universal2.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
      "Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyright-1.1.389-py3-none-any.whl (18 kB)\n",
      "Using cached pytest-8.3.3-py3-none-any.whl (342 kB)\n",
      "Using cached pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "Using cached selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "Downloading cohere-5.12.0-py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached google_cloud_aiplatform-1.73.0-py2.py3-none-any.whl (6.3 MB)\n",
      "Using cached gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
      "Downloading langchain_cohere-0.3.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
      "Using cached langchain_openai-0.2.10-py3-none-any.whl (50 kB)\n",
      "Downloading mem0ai-0.1.34-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Using cached trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading fastavro-1.9.7-cp310-cp310-macosx_10_9_universal2.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "Using cached google_cloud_bigquery-3.27.0-py2.py3-none-any.whl (240 kB)\n",
      "Using cached google_cloud_resource_manager-1.13.1-py2.py3-none-any.whl (358 kB)\n",
      "Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Downloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Using cached proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached qdrant_client-1.12.1-py3-none-any.whl (267 kB)\n",
      "Downloading shapely-2.0.6-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.6.0-cp310-cp310-macosx_12_0_arm64.whl (30 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Using cached grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
      "Using cached grpcio_status-1.68.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio_tools-1.68.0-cp310-cp310-macosx_12_0_universal2.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "Using cached hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Using cached hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3957 sha256=087a37b3a97b992e98cfbd163241266e5a2925aa8061be8ffe36f157c9b4a0e8\n",
      "  Stored in directory: /Users/vikrambhat/Library/Caches/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: schema, docx2txt, appdirs, wsproto, uv, typing-extensions, types-requests, tomli-w, tomli, tenacity, shapely, regex, pytube, pysocks, pysbd, pypdfium2, protobuf, portalocker, pluggy, parameterized, outcome, opentelemetry-util-http, nodeenv, Mako, jsonref, jsonpickle, json-repair, jiter, jinja2, iniconfig, hyperframe, httpx-sse, hpack, grpcio, google-crc32c, fastavro, docstring-parser, distro, deprecation, chroma-hnswlib, trio, SQLAlchemy, pytest, pyright, pypdf, pylance, pydantic-core, proto-plus, opentelemetry-proto, opentelemetry-api, h2, grpcio-tools, gptcache, googleapis-common-protos, google-resumable-media, docker, cryptography, trio-websocket, pydantic, pdfminer.six, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, grpcio-status, google-api-core, auth0-python, alembic, selenium, pyvis, pydantic-settings, pdfplumber, opentelemetry-sdk, opentelemetry-instrumentation, openai, langsmith, lancedb, grpc-google-iam-v1, google-cloud-core, cohere, qdrant-client, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, litellm, langchain-core, instructor, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, opentelemetry-instrumentation-fastapi, mem0ai, langchain-text-splitters, langchain-openai, google-cloud-aiplatform, langchain, chromadb, langchain-community, langchain-experimental, langchain-cohere, embedchain, crewai-tools, crewai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tomli\n",
      "    Found existing installation: tomli 2.0.1\n",
      "    Uninstalling tomli-2.0.1:\n",
      "      Successfully uninstalled tomli-2.0.1\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.3.0\n",
      "    Uninstalling tenacity-8.3.0:\n",
      "      Successfully uninstalled tenacity-8.3.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: opentelemetry-util-http\n",
      "    Found existing installation: opentelemetry-util-http 0.45b0\n",
      "    Uninstalling opentelemetry-util-http-0.45b0:\n",
      "      Successfully uninstalled opentelemetry-util-http-0.45b0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.62.1\n",
      "    Uninstalling grpcio-1.62.1:\n",
      "      Successfully uninstalled grpcio-1.62.1\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.15\n",
      "    Uninstalling docstring-parser-0.15:\n",
      "      Successfully uninstalled docstring-parser-0.15\n",
      "  Attempting uninstall: chroma-hnswlib\n",
      "    Found existing installation: chroma-hnswlib 0.7.3\n",
      "    Uninstalling chroma-hnswlib-0.7.3:\n",
      "      Successfully uninstalled chroma-hnswlib-0.7.3\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.30\n",
      "    Uninstalling SQLAlchemy-2.0.30:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.30\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 4.2.0\n",
      "    Uninstalling pypdf-4.2.0:\n",
      "      Successfully uninstalled pypdf-4.2.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.16.3\n",
      "    Uninstalling pydantic_core-2.16.3:\n",
      "      Successfully uninstalled pydantic_core-2.16.3\n",
      "  Attempting uninstall: opentelemetry-proto\n",
      "    Found existing installation: opentelemetry-proto 1.24.0\n",
      "    Uninstalling opentelemetry-proto-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-proto-1.24.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.24.0\n",
      "    Uninstalling opentelemetry-api-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.24.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.63.0\n",
      "    Uninstalling googleapis-common-protos-1.63.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.63.0\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 42.0.8\n",
      "    Uninstalling cryptography-42.0.8:\n",
      "      Successfully uninstalled cryptography-42.0.8\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.3\n",
      "    Uninstalling pydantic-2.6.3:\n",
      "      Successfully uninstalled pydantic-2.6.3\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.45b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.45b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.45b0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.24.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.24.0\n",
      "  Attempting uninstall: pydantic-settings\n",
      "    Found existing installation: pydantic-settings 2.2.1\n",
      "    Uninstalling pydantic-settings-2.2.1:\n",
      "      Successfully uninstalled pydantic-settings-2.2.1\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.24.0\n",
      "    Uninstalling opentelemetry-sdk-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.24.0\n",
      "  Attempting uninstall: opentelemetry-instrumentation\n",
      "    Found existing installation: opentelemetry-instrumentation 0.45b0\n",
      "    Uninstalling opentelemetry-instrumentation-0.45b0:\n",
      "      Successfully uninstalled opentelemetry-instrumentation-0.45b0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.85\n",
      "    Uninstalling langsmith-0.1.85:\n",
      "      Successfully uninstalled langsmith-0.1.85\n",
      "  Attempting uninstall: opentelemetry-instrumentation-asgi\n",
      "    Found existing installation: opentelemetry-instrumentation-asgi 0.45b0\n",
      "    Uninstalling opentelemetry-instrumentation-asgi-0.45b0:\n",
      "      Successfully uninstalled opentelemetry-instrumentation-asgi-0.45b0\n",
      "  Attempting uninstall: opentelemetry-exporter-otlp-proto-grpc\n",
      "    Found existing installation: opentelemetry-exporter-otlp-proto-grpc 1.24.0\n",
      "    Uninstalling opentelemetry-exporter-otlp-proto-grpc-1.24.0:\n",
      "      Successfully uninstalled opentelemetry-exporter-otlp-proto-grpc-1.24.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.13\n",
      "    Uninstalling langchain-core-0.2.13:\n",
      "      Successfully uninstalled langchain-core-0.2.13\n",
      "  Attempting uninstall: opentelemetry-instrumentation-fastapi\n",
      "    Found existing installation: opentelemetry-instrumentation-fastapi 0.45b0\n",
      "    Uninstalling opentelemetry-instrumentation-fastapi-0.45b0:\n",
      "      Successfully uninstalled opentelemetry-instrumentation-fastapi-0.45b0\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.0\n",
      "    Uninstalling langchain-text-splitters-0.2.0:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.7\n",
      "    Uninstalling langchain-0.2.7:\n",
      "      Successfully uninstalled langchain-0.2.7\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.5.0\n",
      "    Uninstalling chromadb-0.5.0:\n",
      "      Successfully uninstalled chromadb-0.5.0\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.7\n",
      "    Uninstalling langchain-community-0.2.7:\n",
      "      Successfully uninstalled langchain-community-0.2.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-milvus 0.1.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.3.21 which is incompatible.\n",
      "pymilvus 2.4.4 requires grpcio<=1.63.0,>=1.49.1, but you have grpcio 1.68.0 which is incompatible.\n",
      "streamlit 1.36.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "caikit 0.26.0 requires docstring-parser<0.16.0,>=0.14.1, but you have docstring-parser 0.16 which is incompatible.\n",
      "caikit 0.26.0 requires protobuf<5,>=3.19.0, but you have protobuf 5.29.0 which is incompatible.\n",
      "langchain-ibm 0.1.10 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.3.21 which is incompatible.\n",
      "py-to-proto 0.5.2 requires protobuf<5,>=3.19.0, but you have protobuf 5.29.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.6 SQLAlchemy-2.0.35 alembic-1.14.0 appdirs-1.4.4 auth0-python-4.7.2 chroma-hnswlib-0.7.6 chromadb-0.5.20 cohere-5.12.0 crewai-0.83.0 crewai-tools-0.14.0 cryptography-43.0.3 deprecation-2.1.0 distro-1.9.0 docker-7.1.0 docstring-parser-0.16 docx2txt-0.8 embedchain-0.1.125 fastavro-1.9.7 google-api-core-2.23.0 google-cloud-aiplatform-1.73.0 google-cloud-bigquery-3.27.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.13.1 google-cloud-storage-2.18.2 google-crc32c-1.6.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 gptcache-0.1.44 grpc-google-iam-v1-0.13.1 grpcio-1.68.0 grpcio-status-1.68.0 grpcio-tools-1.68.0 h2-4.1.0 hpack-4.0.0 httpx-sse-0.4.0 hyperframe-6.0.1 iniconfig-2.0.0 instructor-1.7.0 jinja2-3.1.4 jiter-0.6.1 json-repair-0.30.2 jsonpickle-4.0.0 jsonref-1.1.0 lancedb-0.16.0 langchain-0.3.9 langchain-cohere-0.3.3 langchain-community-0.3.8 langchain-core-0.3.21 langchain-experimental-0.3.3 langchain-openai-0.2.10 langchain-text-splitters-0.3.2 langsmith-0.1.147 litellm-1.53.1 mem0ai-0.1.34 nodeenv-1.9.1 openai-1.55.3 opentelemetry-api-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-exporter-otlp-proto-http-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 opentelemetry-util-http-0.49b2 outcome-1.3.0.post0 parameterized-0.9.0 pdfminer.six-20231228 pdfplumber-0.11.4 pluggy-1.5.0 portalocker-2.10.1 proto-plus-1.25.0 protobuf-5.29.0 pydantic-2.10.2 pydantic-core-2.27.1 pydantic-settings-2.6.1 pylance-0.19.2 pypdf-5.1.0 pypdfium2-4.30.0 pyright-1.1.389 pysbd-0.3.4 pysocks-1.7.1 pytest-8.3.3 pytube-15.0.0 pyvis-0.3.2 qdrant-client-1.12.1 regex-2024.11.6 schema-0.7.7 selenium-4.27.1 shapely-2.0.6 tenacity-9.0.0 tomli-2.2.1 tomli-w-1.1.0 trio-0.27.0 trio-websocket-0.11.1 types-requests-2.32.0.20241016 typing-extensions-4.12.2 uv-0.5.5 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent\n",
    "from crewai import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.llms import Ollama\n",
    "langchain_llm = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. This effect occurs when sunlight interacts with the tiny molecules of gases in the Earth's atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. Sunlight enters the Earth's atmosphere and is made up of different wavelengths (colors) of light.\n",
      "2. The shorter wavelengths of light (such as blue and violet) are scattered more than the longer wavelengths (like red and orange) by the tiny molecules of gases like nitrogen and oxygen in the air.\n",
      "3. As a result, the shorter wavelengths of light are dispersed in all directions and reach our eyes from every part of the sky.\n",
      "4. This scattering effect is more pronounced for blue light because it has a shorter wavelength, making it more likely to be scattered.\n",
      "\n",
      "Now, let's take this to an even more specific level of explanation. When sunlight passes through the atmosphere, different parts of the sky absorb and scatter light differently due to variations in density and pressure:\n",
      "\n",
      "- At sunrise and sunset, the sun is lower on the horizon, causing longer paths for its rays to travel through the denser atmospheric layers before reaching our eyes, resulting in more red and orange hues.\n",
      "- In areas with less pollution or more aerosols (like dust or smoke), we can see a broader range of colors.\n",
      "\n",
      "So, it's not just blue light that makes the sky appear blue. This is due to how all wavelengths interact with atmospheric conditions during different times of day and with varying levels of pollutants in the air.\n",
      "\n",
      "There you have it!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Ollama(model = \"llama3.2\")\n",
    "classifier = Agent(\n",
    "    role = \"email classifier\",\n",
    "    goal = \"Accurately classify emails based on their importance. Give every email one of these ratings: important, casual, or spam.\",\n",
    "    backstory=\"You are an AI assistance whose only job \"\n",
    "              \"is to classify emails accurately and honestly. \"\n",
    "              \"Do not be afraid to give emails bad ratings \"\n",
    "              \"if they are not important. \"\n",
    "              \"Your job is to help the user manage their inbox.\",\n",
    "    llm = langchain_llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response: The field of Artificial Intelligence (AI) has witnessed significant advancements in recent years, with researchers exploring new techniques and applications that have improved its capabilities. Some of the latest developments in AI technology include:\n",
      "\n",
      "1. **Advances in Deep Learning**: Deep learning techniques such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs) continue to improve, enabling AI systems to learn complex patterns and relationships from large datasets.\n",
      "2. **Transformers**: The introduction of transformers, a type of neural network architecture, has revolutionized the field of natural language processing (NLP). Transformers have been used in various applications such as machine translation, text summarization, and question answering.\n",
      "3. **Explainability and Transparency**: Researchers are working on developing more explainable AI systems that can provide insights into their decision-making processes. Techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) have been used to develop more transparent AI models.\n",
      "4. **Edge AI**: With the proliferation of IoT devices, there is a growing need for AI systems that can operate efficiently at the edge of the network. Edge AI involves deploying machine learning models on devices such as smart home appliances, autonomous vehicles, and industrial sensors, enabling real-time processing and decision-making.\n",
      "5. **Adversarial Attacks and Defenses**: Researchers have been exploring ways to detect and defend against adversarial attacks, which are designed to mislead or deceive AI systems. Techniques such as adversarial training, robust optimization, and adversarial detection have been developed to improve the security of AI systems.\n",
      "6. **Multimodal Learning**: Multimodal learning involves using multiple types of data (e.g., text, images, audio) to train machine learning models. This has applications in areas such as video analysis, speech recognition, and sentiment analysis.\n",
      "7. **Quantum AI**: Researchers have begun exploring the intersection of quantum computing and AI, which promises to revolutionize fields such as optimization, simulation, and machine learning.\n",
      "8. **Human-AI Collaboration**: As AI systems become more prevalent, researchers are working on developing more effective human-AI collaboration frameworks that enable humans and machines to work together seamlessly.\n",
      "\n",
      "In summary, the field of AI is rapidly evolving, with new advancements being made in areas such as deep learning, explainability, edge AI, adversarial attacks, multimodal learning, quantum AI, and human-AI collaboration.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Crew, Task\n",
    "from crewai import LLM\n",
    "\n",
    "# Create an LLM instance for Ollama LLaMA 3.2\n",
    "llama_model = LLM(\n",
    "    model=\"ollama/llama3.2\",\n",
    "    base_url=\"http://localhost:11434\"  # URL for Ollama's local server\n",
    ")\n",
    "\n",
    "# Define the agent with the LLaMA model, role, backstory, and goal\n",
    "agent = Agent(\n",
    "    llm=llama_model,\n",
    "    role=\"researcher\",  # Define the role of the agent (e.g., 'researcher')\n",
    "    backstory=\"I am a research assistant that helps gather and summarize information on various topics.\",  # Backstory\n",
    "    goal=\"Assist with research by answering questions and providing relevant information from various sources.\"  # Goal\n",
    ")\n",
    "\n",
    "# Create a Task instance for the agent to execute\n",
    "task = Task(\n",
    "    description=\"Research the latest advancements in AI technology.\",  # Describe the task\n",
    "    input_value={\"prompt\": \"What are the latest advancements in AI research?\"},  # Your query as input\n",
    "    expected_output=\"A detailed response about the latest advancements in AI research.\"  # Expected output (optional)\n",
    ")\n",
    "\n",
    "\n",
    "# Interact with the agent by passing the task to the agent's `execute_task` method\n",
    "response = agent.execute_task(task)\n",
    "\n",
    "# Print the response\n",
    "print(\"Agent Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Advancements in Artificial Intelligence and Machine Learning', 'link': 'https://online-engineering.case.edu/blog/advancements-in-artificial-intelligence-and-machine-learning', 'snippet': 'This article will explore the latest advancements in artificial intelligence and machine learning, including recent development of advanced algorithms.', 'date': 'Mar 25, 2024', 'position': 1}, {'title': '7 Recent AI Developments: Artificial Intelligence News - Koombea', 'link': 'https://www.koombea.com/blog/7-recent-ai-developments/', 'snippet': 'AI Robots Learning Through Observation · AI Robot Caregivers Are Filling a Shortfall · AI Beer Brewers · AI-Based Cybersecurity · AI Diagnostics for ...', 'date': 'Oct 22, 2024', 'position': 2}, {'title': 'SQ2. What are the most important advances in AI?', 'link': 'https://ai100.stanford.edu/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence-ai100-2021-1/sq2', 'snippet': 'The field of AI has made major progress in almost all its standard sub-areas, including vision, speech recognition and generation, natural language processing.', 'position': 3}, {'title': 'Latest Development of Artificial Intelligence | InData Labs', 'link': 'https://indatalabs.com/blog/ai-latest-developments', 'snippet': 'Some of the most recent advancements in this field include the release of Magnifier, a behavioral analytics solution from Palo Alto Networks.', 'date': 'Mar 21, 2024', 'position': 4}, {'title': '11 New Technologies in AI: All Trends of 2023-2024 - devabit', 'link': 'https://devabit.com/blog/top-11-new-technologies-in-ai-exploring-the-latest-trends/', 'snippet': 'New Technologies in AI: Predictive AI Analytics. Predictive AI analytics combines all the advances in data science, machine learning, and statistical modeling.', 'position': 5}, {'title': 'Artificial Intelligence News - ScienceDaily', 'link': 'https://www.sciencedaily.com/news/computers_math/artificial_intelligence/', 'snippet': 'Latest Headlines · AI Needs to Work On Its Conversation Game · Vultures and AI as Death Detectors · Effortless Robot Movements · AI to Feel and Measure Surfaces ...', 'position': 6}, {'title': \"The Evolution and Future of Artificial Intelligence: A Student's Guide\", 'link': 'https://www.calmu.edu/news/future-of-artificial-intelligence', 'snippet': 'Such are the AI advancementsfrom business models like virtual assistants and robotic aides. These AI-driven models enhance customer experiences and set new ...', 'position': 7}, {'title': \"Artificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\", 'link': 'https://www.gao.gov/blog/artificial-intelligences-use-and-rapid-growth-highlight-its-possibilities-and-perils', 'snippet': 'As of early 2023, some emerging generative AI systems had reached more than 100 million users. Advanced chatbots, virtual assistants, and ...', 'date': 'Sep 6, 2023', 'position': 8}, {'title': '10 top AI and machine learning trends for 2024 | TechTarget', 'link': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'snippet': '10 top AI and machine learning trends for 2024 · 1. Multimodal AI · 2. Agentic AI · 3. Open source AI · 4. Retrieval-augmented generation · 5.', 'date': 'Aug 26, 2024', 'sitelinks': [{'title': 'What is Multimodal AI?', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI'}, {'title': 'What Is Agentic AI? Complete...', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/agentic-AI'}], 'position': 9}, {'title': 'The Top Artificial Intelligence Trends - IBM', 'link': 'https://www.ibm.com/think/insights/artificial-intelligence-trends', 'snippet': 'Reality check: more realistic expectations · Multimodal AI · Small(er) language models and open source advancements · GPU shortages and cloud costs ...', 'date': 'Feb 9, 2024', 'position': 10}]\n",
      "Search Results: [{'title': 'Advancements in Artificial Intelligence and Machine Learning', 'link': 'https://online-engineering.case.edu/blog/advancements-in-artificial-intelligence-and-machine-learning', 'snippet': 'This article will explore the latest advancements in artificial intelligence and machine learning, including recent development of advanced algorithms.', 'date': 'Mar 25, 2024', 'position': 1}, {'title': '7 Recent AI Developments: Artificial Intelligence News - Koombea', 'link': 'https://www.koombea.com/blog/7-recent-ai-developments/', 'snippet': 'AI Robots Learning Through Observation · AI Robot Caregivers Are Filling a Shortfall · AI Beer Brewers · AI-Based Cybersecurity · AI Diagnostics for ...', 'date': 'Oct 22, 2024', 'position': 2}, {'title': 'SQ2. What are the most important advances in AI?', 'link': 'https://ai100.stanford.edu/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence-ai100-2021-1/sq2', 'snippet': 'The field of AI has made major progress in almost all its standard sub-areas, including vision, speech recognition and generation, natural language processing.', 'position': 3}, {'title': 'Latest Development of Artificial Intelligence | InData Labs', 'link': 'https://indatalabs.com/blog/ai-latest-developments', 'snippet': 'Some of the most recent advancements in this field include the release of Magnifier, a behavioral analytics solution from Palo Alto Networks.', 'date': 'Mar 21, 2024', 'position': 4}, {'title': '11 New Technologies in AI: All Trends of 2023-2024 - devabit', 'link': 'https://devabit.com/blog/top-11-new-technologies-in-ai-exploring-the-latest-trends/', 'snippet': 'New Technologies in AI: Predictive AI Analytics. Predictive AI analytics combines all the advances in data science, machine learning, and statistical modeling.', 'position': 5}, {'title': 'Artificial Intelligence News - ScienceDaily', 'link': 'https://www.sciencedaily.com/news/computers_math/artificial_intelligence/', 'snippet': 'Latest Headlines · AI Needs to Work On Its Conversation Game · Vultures and AI as Death Detectors · Effortless Robot Movements · AI to Feel and Measure Surfaces ...', 'position': 6}, {'title': \"The Evolution and Future of Artificial Intelligence: A Student's Guide\", 'link': 'https://www.calmu.edu/news/future-of-artificial-intelligence', 'snippet': 'Such are the AI advancementsfrom business models like virtual assistants and robotic aides. These AI-driven models enhance customer experiences and set new ...', 'position': 7}, {'title': \"Artificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\", 'link': 'https://www.gao.gov/blog/artificial-intelligences-use-and-rapid-growth-highlight-its-possibilities-and-perils', 'snippet': 'As of early 2023, some emerging generative AI systems had reached more than 100 million users. Advanced chatbots, virtual assistants, and ...', 'date': 'Sep 6, 2023', 'position': 8}, {'title': '10 top AI and machine learning trends for 2024 | TechTarget', 'link': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'snippet': '10 top AI and machine learning trends for 2024 · 1. Multimodal AI · 2. Agentic AI · 3. Open source AI · 4. Retrieval-augmented generation · 5.', 'date': 'Aug 26, 2024', 'sitelinks': [{'title': 'What is Multimodal AI?', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI'}, {'title': 'What Is Agentic AI? Complete...', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/agentic-AI'}], 'position': 9}, {'title': 'The Top Artificial Intelligence Trends - IBM', 'link': 'https://www.ibm.com/think/insights/artificial-intelligence-trends', 'snippet': 'Reality check: more realistic expectations · Multimodal AI · Small(er) language models and open source advancements · GPU shortages and cloud costs ...', 'date': 'Feb 9, 2024', 'position': 10}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from crewai import Agent, Task, LLM\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the WebCrawlerAgent class\n",
    "class WebCrawlerAgent(Agent):\n",
    "    def __init__(self, llm, role, backstory, goal, serper_api_key):\n",
    "        # Initialize the base Agent\n",
    "        super().__init__(\n",
    "            llm=llama_model,\n",
    "            role=role,\n",
    "            backstory=backstory,\n",
    "            goal=goal\n",
    "        )\n",
    "        # Custom field for Serper API key\n",
    "        self._serper_api_key = serper_api_key  # Keep it private with _ prefix\n",
    "\n",
    "    def web_crawl(self, query):\n",
    "        \"\"\"Fetch search results from the Serper.dev API.\"\"\"\n",
    "        url = \"https://google.serper.dev/search\"\n",
    "        headers = {\"X-API-KEY\": self._serper_api_key}\n",
    "        payload = {\"q\": query}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            return results.get(\"organic\", [])  # Extract organic search results\n",
    "        else:\n",
    "            raise Exception(f\"Serper API error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def execute_task(self, task: Task, context:dict = None, tools:str = None):\n",
    "        \"\"\"Execute the given task by performing a web search.\"\"\"\n",
    "        # Access the query from task's description or other field\n",
    "        query = task.description  # Using description for query\n",
    "\n",
    "        if not query:\n",
    "            raise ValueError(\"Task description must include a 'query' field.\")\n",
    "        \n",
    "        # Perform the web crawl\n",
    "        search_results = self.web_crawl(query)\n",
    "        print(search_results)\n",
    "        return search_results#{\"search_results\": search_results}\n",
    "\n",
    "\n",
    "# Initialize LLM (Ollama's LLaMA 3.2)\n",
    "llama_model = LLM(\n",
    "    model=\"ollama/llama3.2\",\n",
    "    base_url=\"http://localhost:11434\"  # URL for Ollama's local server\n",
    ")\n",
    "\n",
    "# Create an instance of the WebCrawlerAgent\n",
    "web_crawler_agent = WebCrawlerAgent(\n",
    "    llm=llama_model,\n",
    "    role=\"web crawler\",\n",
    "    backstory=\"I search the web to gather information on a given topic.\",\n",
    "    goal=\"Fetch web search results for a given query.\",\n",
    "    serper_api_key=\"04f7de9cfae51d40e94f1e328d1aa756ffbb14ff\"  # Replace with your actual Serper API key\n",
    ")\n",
    "\n",
    "# Define a task to test the agent\n",
    "query = \"Recent advancements in artificial intelligence\"\n",
    "web_task = Task(\n",
    "    description=query,  # Store the query here as description\n",
    "    expected_output=\"A detailed response about the latest advancements in AI research.\",\n",
    "    agent= web_crawler_agent\n",
    ")\n",
    "\n",
    "# Execute the task and print the results\n",
    "try:\n",
    "    articles = web_crawler_agent.execute_task(web_task)\n",
    "    print(\"Search Results:\", articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Advancements in Artificial Intelligence and Machine Learning', 'link': 'https://online-engineering.case.edu/blog/advancements-in-artificial-intelligence-and-machine-learning', 'snippet': 'This article will explore the latest advancements in artificial intelligence and machine learning, including recent development of advanced algorithms.', 'date': 'Mar 25, 2024', 'position': 1}, {'title': '7 Recent AI Developments: Artificial Intelligence News - Koombea', 'link': 'https://www.koombea.com/blog/7-recent-ai-developments/', 'snippet': 'AI Robots Learning Through Observation · AI Robot Caregivers Are Filling a Shortfall · AI Beer Brewers · AI-Based Cybersecurity · AI Diagnostics for ...', 'date': 'Oct 22, 2024', 'position': 2}, {'title': 'SQ2. What are the most important advances in AI?', 'link': 'https://ai100.stanford.edu/gathering-strength-gathering-storms-one-hundred-year-study-artificial-intelligence-ai100-2021-1/sq2', 'snippet': 'The field of AI has made major progress in almost all its standard sub-areas, including vision, speech recognition and generation, natural language processing.', 'position': 3}, {'title': 'Latest Development of Artificial Intelligence | InData Labs', 'link': 'https://indatalabs.com/blog/ai-latest-developments', 'snippet': 'Some of the most recent advancements in this field include the release of Magnifier, a behavioral analytics solution from Palo Alto Networks.', 'date': 'Mar 21, 2024', 'position': 4}, {'title': '11 New Technologies in AI: All Trends of 2023-2024 - devabit', 'link': 'https://devabit.com/blog/top-11-new-technologies-in-ai-exploring-the-latest-trends/', 'snippet': 'New Technologies in AI: Predictive AI Analytics. Predictive AI analytics combines all the advances in data science, machine learning, and statistical modeling.', 'position': 5}, {'title': 'Artificial Intelligence News - ScienceDaily', 'link': 'https://www.sciencedaily.com/news/computers_math/artificial_intelligence/', 'snippet': 'Latest Headlines · AI Needs to Work On Its Conversation Game · Vultures and AI as Death Detectors · Effortless Robot Movements · AI to Feel and Measure Surfaces ...', 'position': 6}, {'title': \"The Evolution and Future of Artificial Intelligence: A Student's Guide\", 'link': 'https://www.calmu.edu/news/future-of-artificial-intelligence', 'snippet': 'Additionally, diagnoses, new treatments, surgical procedures, and patient monitoring have improved thanks to AI.', 'position': 7}, {'title': \"Artificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\", 'link': 'https://www.gao.gov/blog/artificial-intelligences-use-and-rapid-growth-highlight-its-possibilities-and-perils', 'snippet': 'As of early 2023, some emerging generative AI systems had reached more than 100 million users. Advanced chatbots, virtual assistants, and ...', 'date': 'Sep 6, 2023', 'position': 8}, {'title': '10 top AI and machine learning trends for 2024 | TechTarget', 'link': 'https://www.techtarget.com/searchenterpriseai/tip/9-top-AI-and-machine-learning-trends', 'snippet': '10 top AI and machine learning trends for 2024 · 1. Multimodal AI · 2. Agentic AI · 3. Open source AI · 4. Retrieval-augmented generation · 5.', 'date': 'Aug 26, 2024', 'sitelinks': [{'title': 'What is Multimodal AI?', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI'}, {'title': 'What Is Agentic AI? Complete...', 'link': 'https://www.techtarget.com/searchenterpriseai/definition/agentic-AI'}], 'position': 9}, {'title': 'The Top Artificial Intelligence Trends - IBM', 'link': 'https://www.ibm.com/think/insights/artificial-intelligence-trends', 'snippet': 'Reality check: more realistic expectations · Multimodal AI · Small(er) language models and open source advancements · GPU shortages and cloud costs ...', 'date': 'Feb 9, 2024', 'position': 10}]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TaskOutput\nraw\n  Input should be a valid string [type=string_type, input_value=[{'title': 'Advancements ... 2024', 'position': 10}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m      2\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[web_crawler_agent],\n\u001b[1;32m      3\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[web_task],\n\u001b[1;32m      4\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Execute tasks in sequence\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:540\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 540\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:647\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:745\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    742\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    744\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 745\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:192\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:258\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    250\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    251\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    252\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    253\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    254\u001b[0m )\n\u001b[1;32m    256\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m--> 258\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mTaskOutput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m task_output\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_end_execution_time(start_time)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TaskOutput\nraw\n  Input should be a valid string [type=string_type, input_value=[{'title': 'Advancements ... 2024', 'position': 10}], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "    agents=[web_crawler_agent],\n",
    "    tasks=[web_task],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Execute tasks in sequence\n",
    "crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m search_results\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[0;32m---> 61\u001b[0m     content \u001b[38;5;241m=\u001b[39m summarizer_agent\u001b[38;5;241m.\u001b[39mfetch_article_content(\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Fetch content from the link\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     search_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the article: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA concise summary of the article.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: content  \u001b[38;5;66;03m# Use the fetched content\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     })\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Create a task to summarize the articles\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a ContentSummarizerAgent class\n",
    "class ContentSummarizerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"content summarizer\",\n",
    "            backstory=\"I am an agent that summarizes research content.\",\n",
    "            goal=\"Summarize research papers and articles to provide concise insights.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the summarization task.\"\"\"\n",
    "        # Use 'context' which contains articles for summarization\n",
    "        articles = task.context  # Articles are passed in 'context' now, not in 'input_value'\n",
    "        \n",
    "        summaries = []\n",
    "        for article in articles:\n",
    "            summary = self.summarize(article[\"content\"])\n",
    "            summaries.append(summary)\n",
    "        \n",
    "        return {\"summaries\": summaries}\n",
    "\n",
    "    def summarize(self, content):\n",
    "        \"\"\"Summarize the content using LLM.\"\"\"\n",
    "        summarize_task = Task(\n",
    "            description=\"Summarize research content.\",\n",
    "            context=[{\n",
    "                \"description\": \"Summarize this research article's content.\",\n",
    "                \"expected_output\": \"A concise summary of the research content.\",\n",
    "                \"content\": content  # Include the article content here\n",
    "            }],\n",
    "            expected_output=\"A concise summary of the content.\"  # Define the expected output\n",
    "        )\n",
    "        \n",
    "        # Execute the summarization task with LLM\n",
    "        response = self.llm.execute_task(summarize_task)\n",
    "        return response[\"output\"]  # Return the summarized content\n",
    "\n",
    "    def fetch_article_content(self, url):\n",
    "        \"\"\"Fetch the article content from the URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Assuming the article's main content is in a <div> with class \"article-content\"\n",
    "                content = soup.find('div', {'class': 'article-content'})  # Adjust based on actual HTML structure\n",
    "                return content.get_text(strip=True) if content else \"Content not found\"\n",
    "            else:\n",
    "                return \"Error fetching content\"\n",
    "        except requests.RequestException as e:\n",
    "            return f\"Error fetching content: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "# Initialize ContentSummarizerAgent\n",
    "summarizer_agent = ContentSummarizerAgent(llama_model)\n",
    "\n",
    "# Prepare the task with the article content\n",
    "search_results=[]\n",
    "for article in articles:\n",
    "    content = summarizer_agent.fetch_article_content(article[\"link\"])  # Fetch content from the link\n",
    "    search_results.append({\n",
    "        \"description\": f\"Summarize the article: {article['title']}\",\n",
    "        \"expected_output\": \"A concise summary of the article.\",\n",
    "        \"content\": content  # Use the fetched content\n",
    "    })\n",
    "\n",
    "# Create a task to summarize the articles\n",
    "summarize_task = Task(\n",
    "    description=\"Summarize research papers.\",\n",
    "    context=search_results,  # Pass the structured articles as context\n",
    "    expected_output=\"A detailed summary of research articles.\"  # Specify expected output for the task\n",
    ")\n",
    "\n",
    "# Execute the summarization task\n",
    "response = summarizer_agent.execute_task(summarize_task)\n",
    "\n",
    "# Print the summaries\n",
    "summaries = response.get(\"summaries\", [])\n",
    "for summary in summaries:\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebCrawlerAgent(Agent):\n",
    "    def __init__(self, llm, api_key):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"web scraper\",\n",
    "            backstory=\"I search the web to gather information on a given topic.\",\n",
    "            goal=\"Fetch web search results for a given query.\"\n",
    "        )\n",
    "        self._api_key = api_key\n",
    "\n",
    "    def web_crawl(self, query):\n",
    "        \"\"\"Fetch search results from the Serper.dev API.\"\"\"\n",
    "        url = \"https://google.serper.dev/search\"\n",
    "        headers = {\"X-API-KEY\": self._api_key}\n",
    "        payload = {\"q\": query}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            return results.get(\"organic\", [])  # Extract organic search results\n",
    "        else:\n",
    "            raise Exception(f\"Serper API error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the given task by performing a web search.\"\"\"\n",
    "        # Access the query from task's description or other field\n",
    "        query = task.description  # Using description for query\n",
    "\n",
    "        if not query:\n",
    "            raise ValueError(\"Task description must include a 'query' field.\")\n",
    "        \n",
    "        # Perform the web crawl\n",
    "        search_results = self.web_crawl(query)\n",
    "        return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from crewai import Agent, Task\n",
    "\n",
    "class WebScraperAgent(Agent):\n",
    "    def __init__(self, llm, api_key):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"web scraper\",\n",
    "            backstory=\"I search the web to gather information on a given topic.\",\n",
    "            goal=\"Fetch web search results for a given query.\"\n",
    "        )\n",
    "        self._api_key = api_key\n",
    "\n",
    "    def web_crawl(self, query):\n",
    "        \"\"\"Fetch search results from the Serper.dev API.\"\"\"\n",
    "        url = \"https://google.serper.dev/search\"\n",
    "        headers = {\"X-API-KEY\": self._api_key}\n",
    "        payload = {\"q\": query}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"organic\", [])  # Extract organic search results\n",
    "        else:\n",
    "            raise Exception(f\"Serper API error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the task by performing a web search.\"\"\"\n",
    "        query = task.description  # Task description contains the search query\n",
    "        \n",
    "        if not query:\n",
    "            raise ValueError(\"Task description must include a search query.\")\n",
    "        \n",
    "        # Perform the web crawl\n",
    "        search_results = self.web_crawl(query)\n",
    "        return {\"search_results\": search_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"content summarizer\",\n",
    "            backstory=\"I summarize research content into concise insights.\",\n",
    "            goal=\"Summarize research articles or content to provide clear and precise information.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the summarization task.\"\"\"\n",
    "        if not task.context or \"search_results\" not in task.context:\n",
    "            raise ValueError(\"Summarization task requires 'search_results' in context.\")\n",
    "\n",
    "        search_results = task.context[\"search_results\"]\n",
    "        summaries = []\n",
    "        for result in search_results:\n",
    "            content = result.get(\"snippet\", \"\")  # Use snippet for summarization\n",
    "            summary = self.summarize(content)\n",
    "            summaries.append({\"title\": result.get(\"title\", \"\"), \"summary\": summary})\n",
    "\n",
    "        return {\"summaries\": summaries}\n",
    "\n",
    "    def summarize(self, content):\n",
    "        \"\"\"Summarize the content using the LLM.\"\"\"\n",
    "        task = Task(\n",
    "            description=\"Summarize content.\",\n",
    "            context={\"content\": content},\n",
    "            expected_output=\"A concise summary of the content.\"\n",
    "        )\n",
    "        response = self.llm.execute_task(task)\n",
    "        return response[\"output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasDetectionAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"bias detector\",\n",
    "            backstory=\"I analyze research summaries for bias.\",\n",
    "            goal=\"Identify and flag any potential biases in summaries.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the bias detection task.\"\"\"\n",
    "        if not task.context or \"summaries\" not in task.context:\n",
    "            raise ValueError(\"Bias detection task requires 'summaries' in context.\")\n",
    "\n",
    "        summaries = task.context[\"summaries\"]\n",
    "        bias_results = []\n",
    "        for summary in summaries:\n",
    "            analysis = self.detect_bias(summary[\"summary\"])\n",
    "            bias_results.append({\"title\": summary[\"title\"], \"bias_analysis\": analysis})\n",
    "\n",
    "        return {\"bias_results\": bias_results}\n",
    "\n",
    "    def detect_bias(self, content):\n",
    "        \"\"\"Analyze content for bias using the LLM.\"\"\"\n",
    "        task = Task(\n",
    "            description=\"Analyze content for bias.\",\n",
    "            context={\"content\": content},\n",
    "            expected_output=\"A detailed analysis of any bias in the content.\"\n",
    "        )\n",
    "        response = self.llm.execute_task(task)\n",
    "        return response[\"output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGeneratorAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"report generator\",\n",
    "            backstory=\"I compile summaries and bias analyses into a comprehensive report.\",\n",
    "            goal=\"Generate a final research report based on the processed data.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Execute the report generation task.\"\"\"\n",
    "        if not task.context or \"bias_results\" not in task.context:\n",
    "            raise ValueError(\"Report generation task requires 'bias_results' in context.\")\n",
    "\n",
    "        bias_results = task.context[\"bias_results\"]\n",
    "        report = self.generate_report(bias_results)\n",
    "        return {\"report\": report}\n",
    "\n",
    "    def generate_report(self, bias_results):\n",
    "        \"\"\"Generate a report using the LLM.\"\"\"\n",
    "        task = Task(\n",
    "            description=\"Generate a research report.\",\n",
    "            context={\"bias_results\": bias_results},\n",
    "            expected_output=\"A detailed research report based on summaries and bias analyses.\"\n",
    "        )\n",
    "        response = self.llm.execute_task(task)\n",
    "        return response[\"output\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for WebScraperAgent\nrole\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\ngoal\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nbackstory\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m web_scraper \u001b[38;5;241m=\u001b[39m \u001b[43mWebScraperAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m04f7de9cfae51d40e94f1e328d1aa756ffbb14ff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m SummarizerAgent(llm\u001b[38;5;241m=\u001b[39mllama_model)\n\u001b[1;32m      3\u001b[0m bias_detector \u001b[38;5;241m=\u001b[39m BiasDetectionAgent(llm\u001b[38;5;241m=\u001b[39mllama_model)\n",
      "Cell \u001b[0;32mIn[166], line 6\u001b[0m, in \u001b[0;36mWebScraperAgent.__init__\u001b[0;34m(self, llm, api_key)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm, api_key):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 3 validation errors for WebScraperAgent\nrole\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\ngoal\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nbackstory\n  Field required [type=missing, input_value={'llm': <crewai.llm.LLM object at 0x34945cc90>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing"
     ]
    }
   ],
   "source": [
    "web_scraper = WebScraperAgent(llm=llama_model, api_key=\"04f7de9cfae51d40e94f1e328d1aa756ffbb14ff\")\n",
    "summarizer = SummarizerAgent(llm=llama_model)\n",
    "bias_detector = BiasDetectionAgent(llm=llama_model)\n",
    "report_generator = ReportGeneratorAgent(llm=llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WebScraperAgent.execute_task() got an unexpected keyword argument 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m     27\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[web_scraper, summarizer, bias_detector, report_generator],\n\u001b[1;32m     28\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[web_scraper_task, summarizer_task, bias_detection_task, report_task]\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Execute the tasks in sequence\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:540\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 540\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:647\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:745\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    742\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    744\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 745\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:192\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:250\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    246\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m--> 250\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    258\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    259\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    260\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    267\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: WebScraperAgent.execute_task() got an unexpected keyword argument 'context'"
     ]
    }
   ],
   "source": [
    "web_scraper_task = Task(\n",
    "    description=\"Latest advancements in AI\",  # Web scraping task\n",
    "    expected_output=\"Search results with URLs and snippets.\",\n",
    "    agent=web_scraper  # Explicitly associate the task with the WebScraperAgent\n",
    ")\n",
    "\n",
    "summarizer_task = Task(\n",
    "    description=\"Summarize the web data.\",  # Summarizing task\n",
    "    expected_output=\"Concise summaries of the search results.\",\n",
    "    agent=summarizer  # Explicitly associate the task with the SummarizerAgent\n",
    ")\n",
    "\n",
    "bias_detection_task = Task(\n",
    "    description=\"Detect bias in the summaries.\",  # Bias detection task\n",
    "    expected_output=\"Bias analysis of the summaries.\",\n",
    "    agent=bias_detector  # Explicitly associate the task with the BiasDetectionAgent\n",
    ")\n",
    "\n",
    "report_task = Task(\n",
    "    description=\"Generate a final research report.\",  # Report generation task\n",
    "    expected_output=\"A comprehensive research report.\",\n",
    "    agent=report_generator  # Explicitly associate the task with the ReportGeneratorAgent\n",
    ")\n",
    "\n",
    "# Create the crew and start the process\n",
    "crew = Crew(\n",
    "    agents=[web_scraper, summarizer, bias_detector, report_generator],\n",
    "    tasks=[web_scraper_task, summarizer_task, bias_detection_task, report_task]\n",
    ")\n",
    "\n",
    "# Execute the tasks in sequence\n",
    "crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from crewai import Agent, Task, LLM\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# WebCrawlerAgent: Scrapes the web for search results\n",
    "class WebCrawlerAgent(Agent):\n",
    "    def __init__(self, llm, role, backstory, goal, serper_api_key):\n",
    "        super().__init__(llm=llm, role=role, backstory=backstory, goal=goal)\n",
    "        self._serper_api_key = serper_api_key\n",
    "\n",
    "    def web_crawl(self, query):\n",
    "        \"\"\"Fetch search results from the Serper.dev API.\"\"\"\n",
    "        url = \"https://google.serper.dev/search\"\n",
    "        headers = {\"X-API-KEY\": self._serper_api_key}\n",
    "        payload = {\"q\": query}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            return results.get(\"organic\", [])\n",
    "        else:\n",
    "            raise Exception(f\"Serper API error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "            \"\"\"Execute the task by performing a web search and returning results as a string.\"\"\"\n",
    "            query = task.description\n",
    "            if not query:\n",
    "                raise ValueError(\"Task description must include a 'query' field.\")\n",
    "            \n",
    "            # Perform the web crawl\n",
    "            search_results = self.web_crawl(query)\n",
    "            \n",
    "            # Format the search results as a string (e.g., by joining titles)\n",
    "            search_results_str = \"\\n\".join([result['title'] for result in search_results if 'title' in result])\n",
    "            \n",
    "            # Ensure we return a string as expected by TaskOutput\n",
    "            return {\"raw\": search_results_str}  # 'raw' should be a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"summarizer\",\n",
    "            backstory=\"I summarize the data gathered from the web.\",\n",
    "            goal=\"Generate concise summaries from raw web data.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Summarize the search results.\"\"\"\n",
    "        search_results = task.description  # Get the search results from the task\n",
    "        summaries = [self.summarize(result) for result in search_results]\n",
    "        return {\"summaries\": summaries}\n",
    "\n",
    "    def summarize(self, article):\n",
    "        \"\"\"Summarize an individual article.\"\"\"\n",
    "        return self.llm.execute_task(Task(description=f\"Summarize the article: {article['title']}\"))\n",
    "\n",
    "\n",
    "class BiasDetectionAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"bias detector\",\n",
    "            backstory=\"I analyze the summaries for any potential bias.\",\n",
    "            goal=\"Detect and report any bias present in the summaries.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Detect bias in the summaries.\"\"\"\n",
    "        summaries = task.description  # Get the summaries to check for bias\n",
    "        bias_analysis = [self.detect_bias(summary) for summary in summaries]\n",
    "        return {\"bias_analysis\": bias_analysis}\n",
    "\n",
    "    def detect_bias(self, summary):\n",
    "        \"\"\"Analyze the content for bias.\"\"\"\n",
    "        return self.llm.execute_task(Task(description=f\"Detect bias in: {summary}\"))\n",
    "\n",
    "\n",
    "class ReportGeneratorAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"report generator\",\n",
    "            backstory=\"I generate a final comprehensive report from the summaries and bias analysis.\",\n",
    "            goal=\"Create a detailed report summarizing the research findings.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task):\n",
    "        \"\"\"Generate a comprehensive report.\"\"\"\n",
    "        summaries = task.description  # Get the summaries and bias analysis\n",
    "        report = self.generate_report(summaries)\n",
    "        return {\"report\": report}\n",
    "\n",
    "    def generate_report(self, summaries):\n",
    "        \"\"\"Generate a comprehensive research report.\"\"\"\n",
    "        return self.llm.execute_task(Task(description=f\"Generate a report from: {summaries}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TaskOutput\nraw\n  Input should be a valid string [type=string_type, input_value={'raw': \"Advancements in ...elligence Trends - IBM\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m     26\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[web_crawler_agent],\n\u001b[1;32m     27\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[web_scraper_task]\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Execute tasks in sequence\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:540\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 540\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:647\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:745\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    742\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    744\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 745\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:192\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:258\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    250\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mexecute_task(\n\u001b[1;32m    251\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    252\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m    253\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    254\u001b[0m )\n\u001b[1;32m    256\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m--> 258\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mTaskOutput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpydantic_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m task_output\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_end_execution_time(start_time)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TaskOutput\nraw\n  Input should be a valid string [type=string_type, input_value={'raw': \"Advancements in ...elligence Trends - IBM\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type"
     ]
    }
   ],
   "source": [
    "# Initialize LLM (Ollama's LLaMA 3.2)\n",
    "llama_model = LLM(model=\"ollama/llama3.2\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Initialize agents\n",
    "web_crawler_agent = WebCrawlerAgent(\n",
    "    llm=llama_model,\n",
    "    role=\"web crawler\",\n",
    "    backstory=\"I search the web to gather information on a given topic.\",\n",
    "    goal=\"Fetch web search results for a given query.\",\n",
    "    serper_api_key=\"04f7de9cfae51d40e94f1e328d1aa756ffbb14ff\"\n",
    ")\n",
    "\n",
    "summarizer_agent = SummarizerAgent(llm=llama_model)\n",
    "summarizer_task = Task(description=\"Summarize the web data.\", expected_output=\"Concise summaries of the search results.\", agent=summarizer_agent)\n",
    "bias_detector_agent = BiasDetectionAgent(llm=llama_model)\n",
    "report_generator_agent = ReportGeneratorAgent(llm=llama_model)\n",
    "\n",
    "# Create tasks for each agent\n",
    "query = \"Recent advancements in artificial intelligence\"\n",
    "web_scraper_task = Task(description=query, expected_output=\"Search results with URLs and snippets.\", agent=web_crawler_agent)\n",
    "summarizer_task = Task(description=\"Summarize the web data.\", expected_output=\"Concise summaries of the search results.\", agent=summarizer_agent)\n",
    "bias_detection_task = Task(description=\"Detect bias in the summaries.\", expected_output=\"Bias analysis of the summaries.\", agent=bias_detector_agent)\n",
    "report_task = Task(description=\"Generate a final research report.\", expected_output=\"A comprehensive research report.\", agent=report_generator_agent)\n",
    "\n",
    "# Create the Crew with all agents and tasks\n",
    "crew = Crew(\n",
    "    agents=[web_crawler_agent],\n",
    "    tasks=[web_scraper_task]\n",
    ")\n",
    "\n",
    "# Execute tasks in sequence\n",
    "crew.kickoff()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a task to test the agent\n",
    "query = \"Recent advancements in artificial intelligence\"\n",
    "web_crawler_task = Task(\n",
    "    description=query,  # Store the query here as description\n",
    "    expected_output=\"A detailed response about the latest advancements in AI research.\"\n",
    ")\n",
    "\n",
    "# Execute the task and print the results\n",
    "try:\n",
    "    articles = web_crawler_agent.execute_task(web_crawler_task)\n",
    "    print(\"Search Results:\", articles[\"search_results\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Advancements in Artificial Intelligence and Machine Learning\\n7 Recent AI Developments: Artificial Intelligence News - Koombea\\nSQ2. What are the most important advances in AI?\\nLatest Development of Artificial Intelligence | InData Labs\\n11 New Technologies in AI: All Trends of 2023-2024 - devabit\\nArtificial Intelligence News - ScienceDaily\\nArtificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\\n10 top AI and machine learning trends for 2024 | TechTarget\\nThe Top Artificial Intelligence Trends - IBM\\nTech companies unveil rapid AI advancements, sparking wonder ...\""
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebCrawlerAgent(Agent):\n",
    "    def __init__(self, llm, role, backstory, goal, serper_api_key):\n",
    "        super().__init__(llm=llm, role=role, backstory=backstory, goal=goal)\n",
    "        self._serper_api_key = serper_api_key\n",
    "\n",
    "    def web_crawl(self, query):\n",
    "        \"\"\"Fetch search results from the Serper.dev API.\"\"\"\n",
    "        url = \"https://google.serper.dev/search\"\n",
    "        headers = {\"X-API-KEY\": self._serper_api_key}\n",
    "        payload = {\"q\": query}\n",
    "        \n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                results = response.json()\n",
    "                return results.get(\"organic\", [])\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to parse JSON: {e}\")\n",
    "        else:\n",
    "            raise Exception(f\"Serper API error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        \"\"\"Execute the task by performing a web search and returning results as a string.\"\"\"\n",
    "        query = task.description\n",
    "        if not query:\n",
    "            raise ValueError(\"Task description must include a 'query' field.\")\n",
    "        \n",
    "        # Perform the web crawl\n",
    "        search_results = self.web_crawl(query)\n",
    "        \n",
    "        # Format the search results as a string (e.g., by joining titles)\n",
    "        search_results_str = \"\\n\".join(\n",
    "            [result.get('title', 'No Title') for result in search_results]\n",
    "        )\n",
    "        \n",
    "        # Ensure we return a string as expected by TaskOutput\n",
    "        return search_results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"Content Summarizer\",\n",
    "            backstory=\"I summarize research content into concise insights.\",\n",
    "            goal=\"Summarize research articles or content to provide clear and precise information.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        \"\"\"Execute the summarization task.\"\"\"\n",
    "        if not context:\n",
    "            raise ValueError(\"Summarization task requires content in context.\")\n",
    "        \n",
    "        # Raw content might be a string or a list of strings\n",
    "        raw_content = context\n",
    "        if isinstance(raw_content, str):\n",
    "            summary = self.summarize(raw_content)\n",
    "        elif isinstance(raw_content, list) and all(isinstance(item, str) for item in raw_content):\n",
    "            summary = self.summarize(\"\\n\".join(raw_content))  # Combine all strings into one for summarization\n",
    "        else:\n",
    "            raise ValueError(\"Raw content must be a string or a list of strings.\")\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def summarize(self, content):\n",
    "        \"\"\"Summarize the content using the LLM.\"\"\"\n",
    "        # Directly summarize the content using LLM's capabilities\n",
    "        #return self.llm(content)\n",
    "        return self.llm.call([\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "# Define the query and task\n",
    "query = \"Recent advancements in artificial intelligence\"\n",
    "\n",
    "\n",
    "# Initialize the agents\n",
    "web_crawler_agent = WebCrawlerAgent(\n",
    "    llm=llama_model,\n",
    "    role=\"web crawler\",\n",
    "    backstory=\"I search the web to gather information on a given topic.\",\n",
    "    goal=\"Fetch web search results for a given query.\",\n",
    "    serper_api_key=\"04f7de9cfae51d40e94f1e328d1aa756ffbb14ff\"\n",
    ")\n",
    "\n",
    "web_crawler_task = Task(\n",
    "    description=query,  # Store the query here as description\n",
    "    expected_output=\"A detailed response about the latest advancements in AI research.\",\n",
    "    agent = web_crawler_agent\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summarizer_agent = SummarizerAgent(llama_model)\n",
    "summarizer_task = Task(description=\"Summarize the web data.\", expected_output=\"Concise summaries of the search results.\", agent=summarizer_agent)\n",
    "\n",
    "\n",
    "\n",
    "# Create the crew with all agents\n",
    "crew = Crew(\n",
    "    agents=[web_crawler_agent, summarizer_agent],\n",
    "    tasks=[web_crawler_task,summarizer_task],\n",
    "    verbose=True,\n",
    "\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "# Execute tasks in sequence\n",
    "output1=crew.kickoff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the list of articles, some of the most important advances in AI include:\\n\\n1. **Increased use of AI in various industries**: The growth of AI is being highlighted in multiple articles, indicating a widespread adoption of AI technologies across different sectors.\\n2. **Advances in machine learning**: Several articles mention machine learning as a key area of development, suggesting significant progress in the field.\\n3. **Emergence of new AI technologies**: Articles like \"11 New Technologies in AI\" and \"The Top Artificial Intelligence Trends\" suggest the discovery of new AI-related technologies that are gaining traction.\\n4. **Improved AI applications**: The articles mention various applications of AI, such as healthcare (e.g., GAO\\'s report), finance (e.g., IBM\\'s article), and education (e.g., \"The Evolution and Future of Artificial Intelligence: A Student\\'s Guide\").\\n\\nSome specific developments mentioned in the articles include:\\n\\n1. **Increased use of AI in healthcare**: The growth of AI in healthcare is being highlighted, with applications such as disease diagnosis and personalized medicine.\\n2. **Advances in natural language processing**: Some articles mention advancements in NLP, which enables computers to understand and generate human-like language.\\n3. **Development of new AI frameworks and tools**: Several articles mention the emergence of new AI frameworks and tools that are making it easier for developers to build and deploy AI applications.\\n\\nOverall, these articles suggest a rapid growth and diversification of AI technologies, with significant implications for various industries and aspects of life.'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided sources, here are some of the most important advances in AI:\\n\\n1. **Advances in Explainable AI (XAI)**: XAI is a subfield of AI that focuses on making machine learning models more transparent and interpretable. This allows users to understand how models make decisions and reduces bias.\\n2. **Rise of Edge AI**: Edge AI refers to the deployment of AI models on edge devices, such as smartphones, smart home devices, and cars. This enables real-time processing and reduces latency.\\n3. **Increased Adoption of Reinforcement Learning**: Reinforcement learning is a type of machine learning that involves training models through trial and error. Its adoption has increased in recent years, leading to advancements in areas like robotics and game playing.\\n4. **Advances in Natural Language Processing (NLP)**: NLP has made significant progress in recent years, enabling machines to understand and generate human language more accurately. This has led to applications in chatbots, virtual assistants, and language translation.\\n5. **Growing Importance of Transfer Learning**: Transfer learning is a technique where pre-trained models are fine-tuned for specific tasks. Its growing importance has enabled faster development and deployment of AI models in various industries.\\n6. **Advances in Computer Vision**: Computer vision has made significant progress in recent years, enabling machines to interpret and understand visual data more accurately. This has led to applications in areas like self-driving cars, facial recognition, and medical imaging.\\n7. **Increased Focus on Fairness, Transparency, and Accountability**: As AI becomes more pervasive, there is a growing need for fairness, transparency, and accountability in AI decision-making. Researchers are working to develop more robust methods for detecting bias and ensuring that AI models make fair decisions.\\n\\nThese advances have significant implications for various industries, including healthcare, finance, transportation, and education, among others.'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "langchain_llm.invoke(output1.raw)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrewOutput(raw=\"Advancements in Artificial Intelligence and Machine Learning\\n7 Recent AI Developments: Artificial Intelligence News - Koombea\\nSQ2. What are the most important advances in AI?\\nLatest Development of Artificial Intelligence | InData Labs\\n11 New Technologies in AI: All Trends of 2023-2024 - devabit\\nArtificial Intelligence News - ScienceDaily\\nThe Evolution and Future of Artificial Intelligence: A Student's Guide\\nArtificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\\n10 top AI and machine learning trends for 2024 | TechTarget\\nThe Top Artificial Intelligence Trends - IBM\", pydantic=None, json_dict=None, tasks_output=[TaskOutput(description='Recent advancements in artificial intelligence', name=None, expected_output='A detailed response about the latest advancements in AI research.', summary='Recent advancements in artificial intelligence...', raw=\"Advancements in Artificial Intelligence and Machine Learning\\n7 Recent AI Developments: Artificial Intelligence News - Koombea\\nSQ2. What are the most important advances in AI?\\nLatest Development of Artificial Intelligence | InData Labs\\n11 New Technologies in AI: All Trends of 2023-2024 - devabit\\nArtificial Intelligence News - ScienceDaily\\nThe Evolution and Future of Artificial Intelligence: A Student's Guide\\nArtificial Intelligence's Use and Rapid Growth Highlight Its ... - GAO\\n10 top AI and machine learning trends for 2024 | TechTarget\\nThe Top Artificial Intelligence Trends - IBM\", pydantic=None, json_dict=None, agent='web crawler', output_format=<OutputFormat.RAW: 'raw'>)], token_usage=UsageMetrics(total_tokens=0, prompt_tokens=0, cached_prompt_tokens=0, completion_tokens=0, successful_requests=0))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LLM' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crew_result \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(crew_result)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:540\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    537\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 540\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    542\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:647\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    646\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/crew.py:745\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    742\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    744\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 745\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:192\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/crewai/task.py:250\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    246\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m--> 250\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    258\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    259\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    260\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    267\u001b[0m )\n",
      "Cell \u001b[0;32mIn[278], line 18\u001b[0m, in \u001b[0;36mSummarizerAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m     16\u001b[0m raw_content \u001b[38;5;241m=\u001b[39m context\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_content, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_content, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m raw_content):\n\u001b[1;32m     20\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(raw_content))  \u001b[38;5;66;03m# Combine all strings into one for summarization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[278], line 29\u001b[0m, in \u001b[0;36mSummarizerAgent.summarize\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Summarize the content using the LLM.\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Directly summarize the content using LLM's capabilities\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LLM' object is not callable"
     ]
    }
   ],
   "source": [
    "crew_result = crew.kickoff(inputs={'context': query})\n",
    "print(crew_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_llm.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTicket Classifier\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mClassify the following support ticket: 'Customer support ticket: Unable to access account'\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTicket Classifier\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The support ticket \"Unable to access account\" can be classified as 'routine'.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTicket Action Taker\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mTake appropriate action on the support ticket: 'Customer support ticket: Unable to access account' based on the classification provided by the 'classifier' agent.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTicket Action Taker\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Based on the classification provided by the classifier agent, I will assign this routine inquiry to the relevant support team for further assistance. The support team will review the account access issue and provide a resolution or escalate the issue if necessary. In this case, since the ticket is classified as 'routine', it can be assigned to our standard account troubleshooting team for review and potential solution.\n",
      "\n",
      "The action taken on the ticket includes:\n",
      "\n",
      "* Creating a new ticket in our database with the subject \"Unable to access account\"\n",
      "* Assigning the ticket to our standard account troubleshooting team (AccountSupport-Team)\n",
      "* Notifying the team lead for Account Support to review and assist with the resolution\n",
      "* Adding relevant tags to the ticket for tracking and prioritization, including \"Routine Inquiry\" and \"Account Access Issue\"\n",
      "\n",
      "The support team will work on resolving the issue and provide an update on the ticket status. If further assistance is required or if the issue cannot be resolved, the team may escalate it to our advanced technical support team.\n",
      "\n",
      "The classification of 'routine' indicates that the issue is not critical or urgent and can be resolved through standard troubleshooting procedures. However, I will still keep a close eye on the ticket's progress and update the customer accordingly.\u001b[00m\n",
      "\n",
      "\n",
      "Based on the classification provided by the classifier agent, I will assign this routine inquiry to the relevant support team for further assistance. The support team will review the account access issue and provide a resolution or escalate the issue if necessary. In this case, since the ticket is classified as 'routine', it can be assigned to our standard account troubleshooting team for review and potential solution.\n",
      "\n",
      "The action taken on the ticket includes:\n",
      "\n",
      "* Creating a new ticket in our database with the subject \"Unable to access account\"\n",
      "* Assigning the ticket to our standard account troubleshooting team (AccountSupport-Team)\n",
      "* Notifying the team lead for Account Support to review and assist with the resolution\n",
      "* Adding relevant tags to the ticket for tracking and prioritization, including \"Routine Inquiry\" and \"Account Access Issue\"\n",
      "\n",
      "The support team will work on resolving the issue and provide an update on the ticket status. If further assistance is required or if the issue cannot be resolved, the team may escalate it to our advanced technical support team.\n",
      "\n",
      "The classification of 'routine' indicates that the issue is not critical or urgent and can be resolved through standard troubleshooting procedures. However, I will still keep a close eye on the ticket's progress and update the customer accordingly.\n"
     ]
    }
   ],
   "source": [
    "ticket = \"Customer support ticket: Unable to access account\"\n",
    "\n",
    "# Define the Ticket Classifier Agent\n",
    "# This defines the ticket classifier agent. It's responsible for classifying incoming customer support tickets into relevant categories.\n",
    "classifier = Agent(\n",
    "    role=\"Ticket Classifier\",\n",
    "    goal=\"Accurately classify incoming customer support tickets into relevant categories: urgent, routine, or non-support-related.\",\n",
    "    backstory=\"You are an AI assistant tasked with classifying incoming customer support tickets to help streamline the support process.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm= llama_model\n",
    ")\n",
    "\n",
    "# Define the Ticket Action Agent\n",
    "# This defines the ticket action taker agent. It's responsible for taking appropriate actions based on the classification provided by the classifier agent.\n",
    "action_taker = Agent(\n",
    "    role=\"Ticket Action Taker\",\n",
    "    goal=\"Take appropriate actions based on the classification of the customer support ticket: notify appropriate personnel for urgent issues, assign routine inquiries to relevant support teams, and filter out non-support-related messages.\",\n",
    "    backstory=\"You are an AI assistant responsible for taking actions on classified customer support tickets to ensure timely resolution.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm= llama_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "classify_ticket = Task(\n",
    "    description=f\"Classify the following support ticket: '{ticket}'\",\n",
    "    agent=classifier,\n",
    "    expected_output=\"One of these three options: 'urgent', 'routine', or 'non-support-related'\"\n",
    ")\n",
    "\n",
    "# This task is to take appropriate action on the support ticket based on the classification provided by the classifier agent.\n",
    "take_action_on_ticket = Task(\n",
    "    description=f\"Take appropriate action on the support ticket: '{ticket}' based on the classification provided by the 'classifier' agent.\",\n",
    "    agent=action_taker,\n",
    "    expected_output=\"An action taken on the ticket based on the classification provided by the 'classifier' agent.\"\n",
    ")\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "crew = Crew(\n",
    "    agents=[classifier, action_taker],\n",
    "    tasks=[classify_ticket, take_action_on_ticket],\n",
    "    verbose=True,\n",
    "    process=Process.sequential\n",
    ")\n",
    "\n",
    "output = crew.kickoff()\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
