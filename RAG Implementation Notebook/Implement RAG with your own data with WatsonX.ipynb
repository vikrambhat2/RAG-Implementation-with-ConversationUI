{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot with Custom Data\n",
    "\n",
    "This notebook guides you through the creation of a chatbot tailored to your specific data needs. We utilize Elastic Search as our vector storage solution and incorporate the \"meta-llama/llama-2-13b-chat\" model from WatsonX's Large Language Models (LLM). The `langchain` library plays a crucial role in this process, aiding in tasks like chunking documents, indexing data in Elastic Search, managing conversation chains with memory buffers, and crafting prompt templates.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **PDF Content Processing**: When users upload PDF files, the notebook extracts the text, segments it into manageable chunks, and indexes these chunks in Elastic Search using an appropriate `elser` model.\n",
    "- **Data-Driven Query Handling**: Users can pose questions to the chatbot, which searches the indexed data for relevant answers.\n",
    "- **Integrating Elastic Search and WatsonX LLMs**: We leverage `langchain`'s capabilities to link Elastic Search indexing with WatsonX's LLMs, enabling a seamless conversational experience with memory and retrieval functionalities.\n",
    "- **Hallucination Check**: The notebook includes a mechanism to detect and correct any hallucinations or inaccuracies in the LLM's responses.\n",
    "\n",
    "### Prerequisites for Running the Notebook:\n",
    "\n",
    "\n",
    "1. **Library Requirements**: Confirm that you have installed all libraries specified in the `requirements.txt` file.\n",
    "2. **Elastic Search ELSER Model Setup**: Implement an ELSER model within your Elastic Search instance. Refer to the Elastic documentation for setup details: [Elastic Machine Learning: ELSER](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html).\n",
    "3. **Environment Configuration**: A `.env` file is required, containing critical configuration details:\n",
    "\n",
    "   - **`elastic_search_url`**: This URL connects you to your Elastic Search instance, a search engine built on the Lucene library, offering distributed, multitenant capabilities for full-text search with a web interface and JSON document handling. The `elastic_search_url` serves as your point of interaction for tasks like data indexing and querying.\n",
    "   - **`elastic_search_api_key`**: A key for secure access to your Elastic Search instance, this API key is essential for authentication and authorization, ensuring that only permitted users and applications interact with your Elastic Search server.\n",
    "   - **`WATSONX_APIKEY`**: Your access key for IBM's WatsonX services, this API key is used for authenticating requests to WatsonX's AI and cognitive computing services. Acquire your WatsonX.ai URL and API key by following these instructions: [IBM Cloud: API Keys](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key) and [WatsonX as a Service Documentation](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=library-credentials).\n",
    "   - **`WATSONX_URL`**: The primary access point for WatsonX's API, this URL is where you connect to utilize WatsonX's diverse AI functionalities.\n",
    "   - **`WATSONX_Project_ID`**: A unique identifier for your project in the WatsonX environment, this ID helps manage and organize resources like datasets and AI models within your specified WatsonX project.Obtain a project ID for WatsonX AI, essential for project management within WatsonX. Guides for this can be found here: [Creating a WatsonX Project](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=projects-creating-project), [Finding Your Project ID in IBM Cloud](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-project-id.html?context=wx).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.vectorstores.elasticsearch import ElasticsearchStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import PyPDF2\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain_community.llms import WatsonxLLM\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import panel as pn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f2c32",
   "metadata": {},
   "source": [
    "### User Inputs \n",
    "User can specify/update below inputs as per their needs. \n",
    "\n",
    "\n",
    "`es_model_id`: This refers to the unique identifier of the Elastic Search (ES) model that's being used. Elastic Search models, like the ELSER model, are deployed within the Elastic Search environment to perform specific tasks such as text analysis, natural language processing, or vector search. The es_model_id helps in identifying and referencing the specific model deployed in your Elastic Search instance.\n",
    "\n",
    "`index_name`: In the context of Elastic Search, an index_name denotes the name of the index where your data is stored. The index_name is used to specify which collection of documents you're querying or modifying in your Elastic Search operations. If the index is not already present, one with this name gets created during runtime.\n",
    "\n",
    "`llm_model_id`: This is the identifier for the Large Language Model (LLM) from WatsonX that you're using. WatsonX provides various AI models, including LLMs for different tasks like conversation, text completion, or language translation. The llm_model_id allows you to specify which of these models you want to interact with in your application.\n",
    "\n",
    "`wx_url`: This variable represents the URL for the WatsonX service. WatsonX, being a cloud-based service, can be accessed through its dedicated URL. This URL is used to make API requests, authenticate your application, and access the services provided by WatsonX, like their LLMs or other AI functionalities.\n",
    "\n",
    "`wx_project_id`: The wx_project_id is a unique identifier for a project within the WatsonX ecosystem. In WatsonX, a project is a workspace where you can organize resources, data, models, and other assets. Each project has a unique ID which is used to access and manage the resources within that specific project. This ID ensures that your interactions with the WatsonX API are scoped and managed within the right project context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b513f0bbb83996e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:06.281695Z",
     "start_time": "2024-01-07T10:24:06.277326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "es_model_id = '.elser_model_2_linux-x86_64'\n",
    "index_name = \"elser_index_vb_test_2\"\n",
    "llm_model_id = \"meta-llama/llama-2-13b-chat\" #\"ibm/granite-13b-chat-v2\"#\n",
    "wx_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "wx_project_id = os.environ[\"WATSONX_Project_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "### Enter your pdf file name below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57efcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs=[\"Industry accelerators - IBM Documentation.pdf\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare above documents and their metadata\n",
    "The prepare_docs function below processes a list of PDF documents by extracting text from each page and organizing it into two lists: one for the text content and another for the metadata (titles). It iterates through each page of each PDF, extracts the text, and forms a title using the PDF name and page number. The function returns these two lists, making it useful for indexing and referencing the content of multiple PDFs at a page level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_docs(pdf_docs):\n",
    "    docs = []\n",
    "    metadata = []\n",
    "    content = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "        for index, text in enumerate(pdf_reader.pages):\n",
    "            doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
    "                        'content': pdf_reader.pages[index].extract_text()}\n",
    "            docs.append(doc_page)\n",
    "    for doc in docs:\n",
    "        content.append(doc[\"content\"])\n",
    "        metadata.append({\n",
    "            \"_id\": doc[\"title\"]\n",
    "        })\n",
    "    print(\"Content and metadata are extracted from the documents\")\n",
    "    return content, metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921288be",
   "metadata": {},
   "source": [
    "### Step 2: Chunk the documents \n",
    "The get_text_chunks function takes text content and metadata as inputs and splits the content into smaller chunks. It uses a RecursiveCharacterTextSplitter configured with a specified chunk size (512 characters) and overlap (256 characters) for this purpose. The function processes the content, splitting it into passages while maintaining associated metadata. After splitting, it prints the total number of passages created and returns these split documents. This function is useful for breaking down large text into more manageable, indexed segments for easier processing and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e8895acb436e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:47:25.155847Z",
     "start_time": "2024-01-07T07:47:25.149629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text_chunks(content, metadata):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Elastic Search \n",
    "The ingest_and_get_vector_store function initializes and populates an Elasticsearch vector store with provided document chunks (split_docs). It creates an ElasticsearchStore instance using environment variables for the Elastic Search URL, API key, index name, and a retrieval strategy based on a specified Elastic Search model ID (es_model_id). The function then ingests the split documents into this Elasticsearch store. After processing, it returns the populated vector_store, enabling the storage and retrieval of document vectors for efficient search and analysis. This function essentially sets up and populates an Elasticsearch-based vector store tailored for handling segmented document data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "956e5ad8704a563f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:47:34.333240Z",
     "start_time": "2024-01-07T07:47:34.330872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ingest_and_get_vector_store(split_docs):\n",
    "    vector_store = ElasticsearchStore(\n",
    "                    es_cloud_id= os.environ[\"elastic_search_cloud_id\"],\n",
    "                    es_api_key=os.environ[\"elastic_search_api_key\"],\n",
    "                    index_name=index_name,\n",
    "                    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(model_id=es_model_id)\n",
    "                    )\n",
    "    documents = vector_store.add_documents(\n",
    "        split_docs)\n",
    "    print(\"Documents indexed and vector Store returned\")\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain` function sets up a conversational chain for a chatbot using a vector store, a language model, and memory management. Key steps include:\n",
    "\n",
    "1. **Setting Up LLM Parameters**: Defines various generation parameters for the WatsonX Large Language Model (LLM), like decoding method, token limits, temperature, and selection criteria (Top-K, Top-P).\n",
    "\n",
    "2. **Initializing WatsonX LLM**: Creates a `WatsonxLLM` instance using the LLM model ID, WatsonX URL, project ID, the specified parameters, and an API key from the environment.\n",
    "\n",
    "3. **Creating a Retriever**: Transforms the provided `vector_store` into a retriever for fetching relevant documents.\n",
    "\n",
    "4. **Preparing a Prompt Template**: Utilizes a prompt template for structuring the queries sent to the LLM. User can create/update their own template below.\n",
    "\n",
    "5. **Setting Up Conversation Memory**: Implements a `ConversationBufferMemory` to manage chat history and output answers.\n",
    "\n",
    "6. **Building the Conversational Chain**: Constructs a `ConversationalRetrievalChain` by combining the LLM, retriever, prompt template, and memory. This chain also returns source documents alongside responses.\n",
    "\n",
    "The function ultimately returns this configured conversation chain, which is essential for handling and responding to user queries effectively in a chatbot, integrating both generative AI and information retrieval capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_template =\"\"\"[INST]You are a helpful, respectful, and honest assistant. \n",
    "Always answer as helpfully as possible, while being safe. Be brief in your answers. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If you don\\\\'\\''t know the answer to a question, please do not share false information. \\n Answer with no more than 150 words, in 2 or 3 sentences. If you cannot base your answer on the given document, please state that you do not have an answer.\\n\\n{question} Answer with no more than 200 words. If you cannot base your answer on the given document, please state that you do not have an answer. do not include a question in your response. dont prompt to make select correct answers[/INST]\"\"\"\n",
    "\n",
    "template = \"\"\"[INST]\n",
    "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
    "- Answer the question based on the provided documents.\n",
    "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
    "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
    "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
    "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
    "- Do not fabricate information or include questions in your responses.\n",
    "- do not prompt to select answers. do not ask me questions\n",
    "\n",
    "{question}\n",
    "\n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def get_conversation_chain(vector_store):\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: \"sample\",\n",
    "        GenParams.MAX_NEW_TOKENS: 100,\n",
    "        GenParams.MIN_NEW_TOKENS: 1,\n",
    "        GenParams.TEMPERATURE: 0.5,\n",
    "        GenParams.TOP_K: 50,\n",
    "        GenParams.TOP_P: 1,\n",
    "    }\n",
    "\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=llm_model_id,\n",
    "        url=wx_url,\n",
    "        project_id=wx_project_id,\n",
    "        params=parameters,\n",
    "        apikey=os.environ[\"WATSONX_APIKEY\"]\n",
    "    )\n",
    "    retriever = vector_store.as_retriever()\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
    "                          (llm=watsonx_llm,\n",
    "                           retriever=retriever,\n",
    "                           #condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                           memory=memory,\n",
    "                           return_source_documents=True))\n",
    "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
    "    return conversation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Detect Hallucination in the LLMs Response\n",
    "The `validate_answer_against_sources` function evaluates the reliability of a response by comparing it with source documents. It works as follows:\n",
    "\n",
    "1. **Model Initialization**: Utilizes the SentenceTransformer model 'all-MiniLM-L6-v2' to generate embeddings.\n",
    "\n",
    "2. **Threshold Setting**: Sets a similarity threshold (here, 0.5) to determine the acceptable level of similarity between the response and source documents.\n",
    "\n",
    "3. **Extracting Source Texts**: Gathers the content of the source documents.\n",
    "\n",
    "4. **Computing Embeddings**: Generates embeddings for both the response answer and the source texts.\n",
    "\n",
    "5. **Calculating Similarity**: Computes cosine similarity scores between the response answer's embedding and the embeddings of each source text.\n",
    "\n",
    "6. **Validity Check**: Checks if any of the similarity scores exceed the set threshold. If yes, it implies that the response is sufficiently similar to at least one of the source documents, suggesting its reliability, and returns `True`. If not, it returns `False`.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_answer_against_sources(response_answer, source_documents):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    similarity_threshold = 0.5  \n",
    "    source_texts = [doc.page_content for doc in source_documents]\n",
    "\n",
    "    answer_embedding = model.encode(response_answer, convert_to_tensor=True)\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.pytorch_cos_sim(answer_embedding, source_embeddings)\n",
    "\n",
    "\n",
    "    if any(score.item() > similarity_threshold for score in cosine_scores[0]):\n",
    "        return True  \n",
    "\n",
    "    return False  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content and metadata are extracted from the documents\n"
     ]
    }
   ],
   "source": [
    "content, metadata = prepare_docs(pdf_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62e99300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 3 passages\n"
     ]
    }
   ],
   "source": [
    "split_docs = get_text_chunks(content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f714b5f5df09e897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:13.506700Z",
     "start_time": "2024-01-07T10:24:12.199767Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexed and vector Store returned\n"
     ]
    }
   ],
   "source": [
    "vectorstore = ingest_and_get_vector_store(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79d503befc592a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:24.130465Z",
     "start_time": "2024-01-07T10:44:22.503570Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Chain created for the LLM using the vector store\n"
     ]
    }
   ],
   "source": [
    "conversation_chain=get_conversation_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  what are industry accelerators?\n",
      "A:   industry accelerators are organizations that help startups grow and succeed in their respective industries.\n",
      "\n",
      "\n",
      "\n",
      "Context:\n",
      "\n",
      "\n",
      "\n",
      "1. Industry accelerators are different from traditional accelerators in that they focus on a specific industry, such as fintech or healthtech, rather than a wide range of industries.\n",
      "\n",
      "2. Industry accelerators often have a strong network of industry experts and mentors who can provide valuable guidance and resources to the startups they support.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"what are industry accelerators?\"\n",
    "response=conversation_chain({\"question\": user_question})\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d849",
   "metadata": {},
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f11a",
   "metadata": {},
   "source": [
    "### Detect and Solve Hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c06c3",
   "metadata": {},
   "source": [
    "The response to the second question appears to be generated by the LLMs and not directly retrieved from the documents, resulting in an answer that seems out of context. To address such instances of misinformation or 'hallucination,' we previously developed the function `validate_answer_against_sources`. We can use this function to cross-check the answer with the source documents to ensure its accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "185c23b841a7cfae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:57.591045Z",
     "start_time": "2024-01-07T10:44:57.442686Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  what are industry accelerators?\n",
      "A:  Sorry, I cannot answer the question based on the given documents\n"
     ]
    }
   ],
   "source": [
    "if response['source_documents']:\n",
    "    response_answer = response['answer']\n",
    "    source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "    is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "    if not is_valid_answer:\n",
    "        response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "else:\n",
    "    response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "print(\"Q: \",user_question)\n",
    "print(\"A: \",response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d98cb2",
   "metadata": {},
   "source": [
    "### Conversation UI\n",
    "This Python code leverages the `panel` library to construct an interactive chatbot interface. It features a text input field where you can pose questions, alongside a primary-styled submission button. Upon submission, the entered query triggers a callback function that fetches responses from a pre-defined `conversation_chain`. The response is validated against source documents for accuracy, and if no relevant information is found, a default message is displayed. The chat's flow, comprising both queries and responses, is dynamically updated and displayed in a markdown pane, designed to support vertical scrolling. This setup not only facilitates user interaction with the chatbot but also ensures the reliability of the information provided, enhancing the overall user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30d21d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.3.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.3.7/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.7/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.3.7/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.3.min.js\", \"https://cdn.holoviz.org/panel/1.3.7/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='2fe7f448-7775-4b94-94c7-7be4f793e099'>\n",
       "  <div id=\"c0d011b7-8f67-45fb-b66f-3be1cce5a9bf\" data-root-id=\"2fe7f448-7775-4b94-94c7-7be4f793e099\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"e825d7ab-9a47-415a-a971-a1a6d97b469c\":{\"version\":\"3.3.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"2fe7f448-7775-4b94-94c7-7be4f793e099\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"13386609-b284-41f6-a322-4e628943de53\",\"attributes\":{\"plot_id\":\"2fe7f448-7775-4b94-94c7-7be4f793e099\",\"comm_id\":\"2345823221da441b98c264cc41f19c8c\",\"client_comm_id\":\"116e7c4f952b41bebbc128f9b69bef9d\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"e825d7ab-9a47-415a-a971-a1a6d97b469c\",\"roots\":{\"2fe7f448-7775-4b94-94c7-7be4f793e099\":\"c0d011b7-8f67-45fb-b66f-3be1cce5a9bf\"},\"root_ids\":[\"2fe7f448-7775-4b94-94c7-7be4f793e099\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "2fe7f448-7775-4b94-94c7-7be4f793e099"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l3/524d7s611rs3hl7hsd6rk25w0000gn/T/ipykernel_49080/3781805467.py:10: PanelDeprecationWarning: 'style' is deprecated and will be removed in version 1.4, use 'styles' instead.\n",
      "  conversation_history = pn.pane.Markdown(\"**Conversation History:**\\n\", width=700, height=200, style={'white-space': 'pre-wrap', 'overflow-y': 'auto'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Watcher(inst=Button(button_type='primary', name='Submit'), cls=<class 'panel.widgets.button.Button'>, fn=<function on_submit at 0x2c3a85d30>, mode='args', onlychanged=False, parameter_names=('clicks',), what='value', queued=False, precedence=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.extension()\n",
    "\n",
    "# Text input for user's question\n",
    "question_input = pn.widgets.TextInput(placeholder='Type your question here...', name='Ask a Question')\n",
    "\n",
    "# Button to submit the question\n",
    "submit_button = pn.widgets.Button(name='Submit', button_type='primary')\n",
    "\n",
    "# Area to display conversation history\n",
    "conversation_history = pn.pane.Markdown(\"**Conversation History:**\\n\", width=700, height=200, style={'white-space': 'pre-wrap', 'overflow-y': 'auto'})\n",
    "\n",
    "def on_submit(event):\n",
    "    user_question = question_input.value\n",
    "    response = conversation_chain({\"question\": user_question})  # Assuming conversation_chain is defined\n",
    "    \n",
    "    if response['source_documents']:\n",
    "        response_answer = response['answer']\n",
    "        source_docs = response['source_documents']\n",
    "\n",
    "    # Post-processing step to validate the answer against the source documents\n",
    "        is_valid_answer = validate_answer_against_sources(response_answer, source_docs)\n",
    "        if not is_valid_answer:\n",
    "            response['answer'] = \"Sorry I can not answer the question based on the given documents\"\n",
    "    else:\n",
    "        response['answer'] =\"Sorry, I cannot answer the question based on the given documents\"\n",
    "\n",
    "\n",
    "    # Update conversation history\n",
    "    new_entry = f\"**Q**: {user_question}\\n**A**: {response['answer']}\\n\\n---\\n\\n\"\n",
    "    conversation_history.object = new_entry + conversation_history.object\n",
    "\n",
    "    # Clear the input box\n",
    "    question_input.value = ''\n",
    "\n",
    "submit_button.on_click(on_submit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "884212c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8e75fb1e3247778ce06090ef28ec6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'6e25e475-3f9b-4001-a441-2b473e863d20': {'version"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interface = pn.Column(\n",
    "    \"# RAG with your own data Chatbot\",\n",
    "    question_input,\n",
    "    submit_button,\n",
    "    conversation_history\n",
    ")\n",
    "\n",
    "chat_interface.servable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb64f65",
   "metadata": {},
   "source": [
    "We have now set up end to end Retrieval Augmented Generation Chatbot using Elastic Search, LangChain and WatsonX. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
