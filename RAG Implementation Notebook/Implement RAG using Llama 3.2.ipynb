{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot in your local with Llama 3.2 using Ollama\n",
    "\n",
    "This notebook demonstrates a streamlined workflow for building a **Retrieval-Augmented Generation (RAG)** system locally using **Llama 3.2** via **Ollama**. The goal is to enable efficient document-based question answering by integrating document ingestion, vector database storage, and a conversational retrieval system. \n",
    "\n",
    "The following steps are covered:\n",
    "\n",
    "1. **Document Preparation and Splitting**:  PDF, CSV, and DOCX files from a specified directory are loaded, and the content is split into smaller, manageable chunks using the RecursiveCharacterTextSplitter. This process ensures optimal chunking based on size and overlap to improve retrieval accuracy during further processing.\n",
    "\n",
    "2. **Ingesting Documents into Vector Database**: The split documents are embedded using `HuggingFaceEmbeddings` and stored in a local FAISS vector database, facilitating fast and scalable document retrieval.\n",
    "\n",
    "3. **Building the Conversation Chain**: A conversational chain is built using **Llama 3.2** to retrieve relevant information based on user queries. This chain ensures that the responses are accurate, concise, and relevant to the context of the chat history, while managing session history to maintain the flow of conversation.\n",
    "\n",
    "4. **Similarity Score Calculation**: To evaluate the relevancy of the systemâ€™s responses, the notebook includes a function to calculate the similarity score between the generated answers and the source documents using `SentenceTransformer`.\n",
    "\n",
    "The notebook is designed for local execution, leveraging the performance and capabilities of Llama 3.2 via the **Ollama** API to offer a robust and efficient Q&A solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d076e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_packages=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc305674",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_packages==True:\n",
    "    !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, CSVLoader, UnstructuredWordDocumentLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#from htmlTemplate import css, bot_template, user_template\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "#### Specify values for the following:\n",
    "\n",
    "- **`file_directory`**: Provide the name of the folder where your PDF,docx or csv files are stored.\n",
    "\n",
    "- **`embedding_model`**: This is the model used to generate text embeddings. It will be applied to both chunked documents and for calculating the similarity score between the source documents and the LLM's response.\n",
    "\n",
    "- **`llm_model`**: Refers to the language model being used,  \"llama3.2\" in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e771c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory=\"data_directory\"\n",
    "embedding_model='sentence-transformers/all-MiniLM-L6-v2'\n",
    "llm_model =\"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare documents and their metadata\n",
    "This function, `prepare_and_split_docs()`, loads documents from a given directory, including PDFs, DOCX files, and CSVs, using `DirectoryLoader` with specific loaders for each file type. It then splits these documents into smaller chunks using a `RecursiveCharacterTextSplitter` with a defined chunk size and overlap. Finally, the function returns the split documents while maintaining metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_split_docs(directory):\n",
    "    # Load the documents\n",
    "    loaders = [\n",
    "        DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\n",
    "        DirectoryLoader(directory, glob=\"**/*.docx\",show_progress=True),\n",
    "        DirectoryLoader(directory, glob=\"**/*.csv\",loader_cls=CSVLoader)\n",
    "    ]\n",
    "\n",
    "\n",
    "    documents=[]\n",
    "    for loader in loaders:\n",
    "        data =loader.load()\n",
    "        documents.extend(data)\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,  # Use the smaller chunk size here to avoid repeating splitting logic\n",
    "        chunk_overlap=256,\n",
    "        disallowed_special=(),\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "\n",
    "    # Split the documents and keep metadata\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Vector Database locally\n",
    "\n",
    "The `ingest_into_vectordb` function is designed for processing and indexing a collection of documents into a vector database using FAISS (Facebook AI Similarity Search) for efficient similarity searches. It operates as follows:\n",
    "\n",
    "1. **Embedding Creation**: It generates embeddings for the input documents (`split_docs`) using the Hugging Face model `'sentence-transformers/all-MiniLM-L6-v2'`. This model is specifically chosen for its efficiency in creating sentence-level embeddings and is set to run on the CPU.\n",
    "\n",
    "2. **Vector Database Indexing**: Utilizes the generated embeddings to create a FAISS vector database. FAISS is used for its ability to efficiently handle large-scale similarity searches and clustering of dense vectors.\n",
    "\n",
    "3. **Local Storage**: After creating the vector database, the function saves it locally to the path specified by `DB_FAISS_PATH`, ensuring the data can be easily accessed for future similarity searches or retrieval tasks.\n",
    "\n",
    "The primary purpose of this function is to transform textual data into a structured, searchable vector format, facilitating efficient and scalable retrieval tasks such as document similarity searches or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2d9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "def ingest_into_vectordb(split_docs):\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(\"Documents are inserted into FAISS vectorstore\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain(retriever)` function creates a stateful conversational RAG system.\n",
    "\n",
    "1. It initializes the `llama3.2` model and defines two prompts:\n",
    "   - A contextualization prompt to handle the user's query in light of the chat history.\n",
    "   - A system prompt for answering concisely with 2-3 sentences based on retrieved documents.\n",
    "\n",
    "2. It builds a `history_aware_retriever` using the retriever, LLM, and the contextualization prompt to ensure responses are context-aware.\n",
    "\n",
    "3. A `question_answer_chain` is set up to respond with answers limited to 50 words.\n",
    "\n",
    "4. These components are combined into a RAG chain using `create_retrieval_chain`.\n",
    "\n",
    "5. To manage chat history across sessions, it defines `get_session_history`, which stores and retrieves message history by session ID.\n",
    "\n",
    "6. Finally, a `RunnableWithMessageHistory` integrates the RAG chain with chat history management, ensuring the bot maintains state and provides contextually relevant responses throughout the conversation.\n",
    "\n",
    "This function sets up a sophisticated conversational AI system combining the LLaMA model for language generation and a vector database for information retrieval, enhanced with a callback manager for additional processing and a conversation memory buffer for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_conversation_chain(retriever):\n",
    "    llm = Ollama(model=llm_model)\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given the chat history and the latest user question, \"\n",
    "        \"provide a response that directly addresses the user's query based on the provided  documents. \"\n",
    "        \"Do not rephrase the question or ask follow-up questions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"As a personal chat assistant, provide accurate and relevant information based on the provided document in 2-3 sentences. \"\n",
    "        \"Answe should be limited to 50 words and 2-3 sentences.  do not prompt to select answers or do not formualate a stand alone question. do not ask questions in the response. \"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "    ### Statefully manage chat history ###\n",
    "    store = {}\n",
    "\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    print(\"Conversational chain created\")\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Document Similarity in the LLMs Response\n",
    "The `calculate_similarity_score` function computes the cosine similarity between a given answer and a list of context documents using Sentence Transformers. It first encodes the answer and context documents into embeddings. Then, it calculates the cosine similarities between the answer embedding and the context embeddings. The function returns the maximum similarity score, indicating how closely the answer relates to the most relevant context document. Scores range from 0 (no similarity) to 1 (perfect similarity), with higher scores reflecting better alignment with the context.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_score(answer: str, context_docs: list) -> float:\n",
    "       \n",
    "    context_docs = [doc.page_content for doc in context_docs]\n",
    "    \n",
    "    # Encode the answer and context documents\n",
    "    answer_embedding = embeddings.embed_query(answer)\n",
    "    context_embeddings = embeddings.embed_documents(context_docs)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = util.pytorch_cos_sim(answer_embedding, context_embeddings)\n",
    "    \n",
    "    # Return the maximum similarity score from the context documents\n",
    "    max_score = similarities.max().item() \n",
    "    return max_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 4 passages\n",
      "Documents are inserted into FAISS vectorstore\n",
      "Conversational chain created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_docs=prepare_and_split_docs(file_directory)\n",
    "vector_db= ingest_into_vectordb(split_docs)\n",
    "retriever =vector_db.as_retriever()\n",
    "conversational_rag_chain=get_conversation_chain(retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e513bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question=\"Can you summarise the documents ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides statistical data for four categories (A, B, C, D) with mean, median, standard deviation, and variance values. The categories appear to represent different groups or populations, with Category A having the lowest mean value of 23.5 and Category D having the highest mean value of 39.9.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa1=conversational_rag_chain.invoke(\n",
    "    {\"input\": user_question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")\n",
    "print(qa1[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c22c53",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be79e26f",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Chain\n",
      "{'input': 'Can you summarise the documents ?', 'chat_history': [], 'context': [Document(metadata={'source': 'data_directory/simple_stats.csv', 'row': 0}, page_content='Category: A\\nMean: 23.5\\nMedian: 22.5\\nStandard Deviation: 5.1\\nVariance: 26.01'), Document(metadata={'source': 'data_directory/simple_stats.csv', 'row': 3}, page_content='Category: D\\nMean: 39.9\\nMedian: 38.5\\nStandard Deviation: 6.9\\nVariance: 47.61'), Document(metadata={'source': 'data_directory/simple_stats.csv', 'row': 1}, page_content='Category: B\\nMean: 45.7\\nMedian: 44.6\\nStandard Deviation: 8.7\\nVariance: 75.69'), Document(metadata={'source': 'data_directory/simple_stats.csv', 'row': 2}, page_content='Category: C\\nMean: 12.8\\nMedian: 12.0\\nStandard Deviation: 2.3\\nVariance: 5.29')], 'answer': 'The document provides statistical data for four categories (A, B, C, D) with mean, median, standard deviation, and variance values. The categories appear to represent different groups or populations, with Category A having the lowest mean value of 23.5 and Category D having the highest mean value of 39.9.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Conversation Chain\")\n",
    "print(qa1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d31975",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "### Calculate Similarity score between the LLM Response and context documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acfdf2f4",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Similarity Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer = qa1[\"answer\"]\n",
    "context_docs = qa1[\"context\"]\n",
    "similarity_score = calculate_similarity_score(answer, context_docs)\n",
    "\n",
    "print(\"Context Similarity Score:\", round(similarity_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018700c",
   "metadata": {},
   "source": [
    "### Sumamry\n",
    "This notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) pipeline using Llama 3.2 via the Ollama API. We began by preparing and splitting PDF, docx or csv documents into manageable chunks, then ingested these into a vector database with FAISS and Hugging Face embeddings for efficient retrieval.\n",
    "\n",
    "We integrated the retriever with a conversation chain driven by an LLM, using customized system prompts to generate concise responses based on relevant documents. Additionally, we implemented a method for managing conversation history to maintain context in multi-turn interactions, along with calculating similarity scores between generated answers and source documents using SentenceTransformers.\n",
    "\n",
    "Overall, this notebook serves as a guide for creating a locally deployable RAG application, effectively combining Llama 3.2 for inference and FAISS for document retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
